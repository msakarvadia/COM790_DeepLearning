{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Batch norm and residual connections - HW 5",
      "provenance": [],
      "collapsed_sections": [
        "tB6FmzSL4XNB",
        "gc3jeFnG4JHa",
        "xK6b4fox-sak"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoAyEW2Yp9B2"
      },
      "source": [
        "import torch\r\n",
        "import torchvision\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYovBDSbwfYD"
      },
      "source": [
        "# 1. Batch normalization (3 points)\n",
        "\n",
        "Please make use of the model code from Homework 4 question 2 as you work on this question.\n",
        "\n",
        "1. If your model didn't use batch normalization, add it. If it already had batch normalization, remove it. How does the performance change? Please experiment with different learning rate values, since batch normalization can allow a different range of learning rates to work. You are welcome to write your own batch norm implementaiton or use a pre-existing one from mxnet or pytorch.\n",
        "1. Plot the squared L2 norm of gradients with respect to parameters over the course of training with and without batch normalization. To do this, you will need to compute the gradient of the loss over some data with respect to each parameter, square these gradients, and sum the result across all parameters. You can compute this value at each training step, or after each epoch. What difference does batch normalization make in terms of the squared L2 norm of gradients?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdHvFEuwp-vp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cb92c9b-7ce1-430c-a801-e663028c6d64"
      },
      "source": [
        "transform = transforms.Compose(\r\n",
        "    [transforms.ToTensor(),\r\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\r\n",
        "\r\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\r\n",
        "                                        download=True, transform=transform)\r\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\r\n",
        "                                          shuffle=True, num_workers=2)\r\n",
        "\r\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\r\n",
        "                                       download=True, transform=transform)\r\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\r\n",
        "                                         shuffle=False, num_workers=2)\r\n",
        "\r\n",
        "classes = ('plane', 'car', 'bird', 'cat',\r\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7969nNu7bkYg"
      },
      "source": [
        "###**Graphs of L2 norms for all three versions of the network are printed at the bottom of the validation loss printing after training.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB6FmzSL4XNB"
      },
      "source": [
        "### W/O Batch norm or Residual Connections (My original network!!)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvvwelO_4XNC"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "# This comes from https://github.com/mtrencseni/pytorch-playground/blob/master/05-cifar-10/CIFAR-10.ipynb\r\n",
        "class Net(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Net, self).__init__()\r\n",
        "        self.conv1 = nn.Conv2d(3,   64,  3)\r\n",
        "        self.conv2 = nn.Conv2d(64,  128, 3)\r\n",
        "        self.conv3 = nn.Conv2d(128, 256, 3)\r\n",
        "        self.pool = nn.MaxPool2d(2, 2)\r\n",
        "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\r\n",
        "        self.fc2 = nn.Linear(128, 256)\r\n",
        "        self.fc3 = nn.Linear(256, 10)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.pool(F.relu(self.conv1(x)))\r\n",
        "        x = self.pool(F.relu(self.conv2(x)))\r\n",
        "        x = self.pool(F.relu(self.conv3(x)))\r\n",
        "        x = x.view(-1, 64 * 4 * 4)\r\n",
        "        x = F.relu(self.fc1(x))\r\n",
        "        x = F.relu(self.fc2(x))\r\n",
        "        x = self.fc3(x)\r\n",
        "        return F.log_softmax(x, dim=1)\r\n",
        "\r\n",
        "net = Net()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhEHESO04XNC"
      },
      "source": [
        "import torch.optim as optim\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9y3qUXVU4XND"
      },
      "source": [
        "Soruces:\r\n",
        "* https://discuss.pytorch.org/t/how-are-optimizer-step-and-loss-backward-related/7350\r\n",
        "* https://discuss.pytorch.org/t/how-to-interpret-the-grad-tensor-in-the-optimizer/34892\r\n",
        "* https://discuss.pytorch.org/t/a-problem-about-optimizer-param-groups-in-step-function/14463/3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m8ActoK14XND",
        "outputId": "0c9df2fa-c675-42c8-e93c-1f41dafce1e7"
      },
      "source": [
        "x_vals = range(1,26)\r\n",
        "l2_norms = []\r\n",
        "\r\n",
        "for epoch in range(25):  # loop over the dataset multiple times\r\n",
        "\r\n",
        "    running_loss = 0.0\r\n",
        "    for i, data in enumerate(trainloader, 0):\r\n",
        "        # get the inputs\r\n",
        "        inputs, labels = data\r\n",
        "\r\n",
        "        # zero the parameter gradients\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        # forward + backward + optimize\r\n",
        "        outputs = net(inputs)\r\n",
        "        loss = criterion(outputs, labels)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        # print statistics\r\n",
        "        running_loss += loss.item()\r\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\r\n",
        "            print('[%d, %5d] loss: %.3f' %\r\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\r\n",
        "            running_loss = 0.0\r\n",
        "\r\n",
        "    #I think this is computing the right l2 norm:\r\n",
        "    #l2_norm = torch.sum(torch.square(optimizer.param_groups[0]['params'][0].grad)).item()\r\n",
        "    #print(l2_norm)\r\n",
        "    #l2_norms.append(l2_norm)\r\n",
        "    \r\n",
        "    grad_norm_sum = 0\r\n",
        "    for p in net.parameters():\r\n",
        "      if p.grad != None:\r\n",
        "        grad_norm_sum += torch.sum(torch.square(p.grad))\r\n",
        "    l2_norms.append(grad_norm_sum)\r\n",
        "    print(grad_norm_sum)\r\n",
        "    # THIS IS IMPORTANT: https://discuss.pytorch.org/t/difference-between-gradients-from-network-parameters-and-register-backward-hook/4580/3\r\n",
        "    # This is also important: https://discuss.pytorch.org/t/get-the-gradient-of-the-network-parameters/50575\r\n",
        "    \r\n",
        "plt.scatter(x_vals, l2_norms)\r\n",
        "plt.show()\r\n",
        "\r\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  2000] loss: 2.210\n",
            "[1,  4000] loss: 1.868\n",
            "[1,  6000] loss: 1.675\n",
            "[1,  8000] loss: 1.542\n",
            "[1, 10000] loss: 1.442\n",
            "[1, 12000] loss: 1.384\n",
            "tensor(30.9988)\n",
            "[2,  2000] loss: 1.276\n",
            "[2,  4000] loss: 1.218\n",
            "[2,  6000] loss: 1.179\n",
            "[2,  8000] loss: 1.141\n",
            "[2, 10000] loss: 1.104\n",
            "[2, 12000] loss: 1.063\n",
            "tensor(292.8187)\n",
            "[3,  2000] loss: 0.949\n",
            "[3,  4000] loss: 0.928\n",
            "[3,  6000] loss: 0.901\n",
            "[3,  8000] loss: 0.902\n",
            "[3, 10000] loss: 0.868\n",
            "[3, 12000] loss: 0.885\n",
            "tensor(75.7725)\n",
            "[4,  2000] loss: 0.741\n",
            "[4,  4000] loss: 0.737\n",
            "[4,  6000] loss: 0.765\n",
            "[4,  8000] loss: 0.774\n",
            "[4, 10000] loss: 0.720\n",
            "[4, 12000] loss: 0.729\n",
            "tensor(143.9673)\n",
            "[5,  2000] loss: 0.601\n",
            "[5,  4000] loss: 0.616\n",
            "[5,  6000] loss: 0.612\n",
            "[5,  8000] loss: 0.626\n",
            "[5, 10000] loss: 0.614\n",
            "[5, 12000] loss: 0.640\n",
            "tensor(287.7019)\n",
            "[6,  2000] loss: 0.487\n",
            "[6,  4000] loss: 0.507\n",
            "[6,  6000] loss: 0.509\n",
            "[6,  8000] loss: 0.521\n",
            "[6, 10000] loss: 0.520\n",
            "[6, 12000] loss: 0.513\n",
            "tensor(269.6904)\n",
            "[7,  2000] loss: 0.376\n",
            "[7,  4000] loss: 0.417\n",
            "[7,  6000] loss: 0.417\n",
            "[7,  8000] loss: 0.443\n",
            "[7, 10000] loss: 0.434\n",
            "[7, 12000] loss: 0.447\n",
            "tensor(254.9917)\n",
            "[8,  2000] loss: 0.311\n",
            "[8,  4000] loss: 0.329\n",
            "[8,  6000] loss: 0.340\n",
            "[8,  8000] loss: 0.352\n",
            "[8, 10000] loss: 0.370\n",
            "[8, 12000] loss: 0.378\n",
            "tensor(13.4455)\n",
            "[9,  2000] loss: 0.241\n",
            "[9,  4000] loss: 0.272\n",
            "[9,  6000] loss: 0.285\n",
            "[9,  8000] loss: 0.294\n",
            "[9, 10000] loss: 0.315\n",
            "[9, 12000] loss: 0.336\n",
            "tensor(104.3725)\n",
            "[10,  2000] loss: 0.205\n",
            "[10,  4000] loss: 0.233\n",
            "[10,  6000] loss: 0.244\n",
            "[10,  8000] loss: 0.259\n",
            "[10, 10000] loss: 0.242\n",
            "[10, 12000] loss: 0.273\n",
            "tensor(41.3188)\n",
            "[11,  2000] loss: 0.166\n",
            "[11,  4000] loss: 0.183\n",
            "[11,  6000] loss: 0.200\n",
            "[11,  8000] loss: 0.218\n",
            "[11, 10000] loss: 0.220\n",
            "[11, 12000] loss: 0.245\n",
            "tensor(19.5439)\n",
            "[12,  2000] loss: 0.149\n",
            "[12,  4000] loss: 0.155\n",
            "[12,  6000] loss: 0.176\n",
            "[12,  8000] loss: 0.187\n",
            "[12, 10000] loss: 0.193\n",
            "[12, 12000] loss: 0.198\n",
            "tensor(0.0983)\n",
            "[13,  2000] loss: 0.111\n",
            "[13,  4000] loss: 0.152\n",
            "[13,  6000] loss: 0.164\n",
            "[13,  8000] loss: 0.154\n",
            "[13, 10000] loss: 0.163\n",
            "[13, 12000] loss: 0.184\n",
            "tensor(19.9486)\n",
            "[14,  2000] loss: 0.086\n",
            "[14,  4000] loss: 0.130\n",
            "[14,  6000] loss: 0.133\n",
            "[14,  8000] loss: 0.149\n",
            "[14, 10000] loss: 0.152\n",
            "[14, 12000] loss: 0.172\n",
            "tensor(5.9148e-05)\n",
            "[15,  2000] loss: 0.090\n",
            "[15,  4000] loss: 0.117\n",
            "[15,  6000] loss: 0.127\n",
            "[15,  8000] loss: 0.130\n",
            "[15, 10000] loss: 0.118\n",
            "[15, 12000] loss: 0.152\n",
            "tensor(176.4814)\n",
            "[16,  2000] loss: 0.071\n",
            "[16,  4000] loss: 0.101\n",
            "[16,  6000] loss: 0.122\n",
            "[16,  8000] loss: 0.113\n",
            "[16, 10000] loss: 0.114\n",
            "[16, 12000] loss: 0.129\n",
            "tensor(314.3602)\n",
            "[17,  2000] loss: 0.066\n",
            "[17,  4000] loss: 0.090\n",
            "[17,  6000] loss: 0.094\n",
            "[17,  8000] loss: 0.107\n",
            "[17, 10000] loss: 0.122\n",
            "[17, 12000] loss: 0.112\n",
            "tensor(0.1685)\n",
            "[18,  2000] loss: 0.064\n",
            "[18,  4000] loss: 0.083\n",
            "[18,  6000] loss: 0.101\n",
            "[18,  8000] loss: 0.088\n",
            "[18, 10000] loss: 0.096\n",
            "[18, 12000] loss: 0.115\n",
            "tensor(0.2336)\n",
            "[19,  2000] loss: 0.038\n",
            "[19,  4000] loss: 0.078\n",
            "[19,  6000] loss: 0.086\n",
            "[19,  8000] loss: 0.086\n",
            "[19, 10000] loss: 0.099\n",
            "[19, 12000] loss: 0.091\n",
            "tensor(722.4344)\n",
            "[20,  2000] loss: 0.073\n",
            "[20,  4000] loss: 0.067\n",
            "[20,  6000] loss: 0.081\n",
            "[20,  8000] loss: 0.090\n",
            "[20, 10000] loss: 0.085\n",
            "[20, 12000] loss: 0.104\n",
            "tensor(1210.9436)\n",
            "[21,  2000] loss: 0.035\n",
            "[21,  4000] loss: 0.047\n",
            "[21,  6000] loss: 0.067\n",
            "[21,  8000] loss: 0.082\n",
            "[21, 10000] loss: 0.085\n",
            "[21, 12000] loss: 0.081\n",
            "tensor(15.2816)\n",
            "[22,  2000] loss: 0.038\n",
            "[22,  4000] loss: 0.075\n",
            "[22,  6000] loss: 0.058\n",
            "[22,  8000] loss: 0.047\n",
            "[22, 10000] loss: 0.068\n",
            "[22, 12000] loss: 0.091\n",
            "tensor(20.4832)\n",
            "[23,  2000] loss: 0.048\n",
            "[23,  4000] loss: 0.046\n",
            "[23,  6000] loss: 0.053\n",
            "[23,  8000] loss: 0.053\n",
            "[23, 10000] loss: 0.072\n",
            "[23, 12000] loss: 0.083\n",
            "tensor(132.9426)\n",
            "[24,  2000] loss: 0.043\n",
            "[24,  4000] loss: 0.038\n",
            "[24,  6000] loss: 0.049\n",
            "[24,  8000] loss: 0.061\n",
            "[24, 10000] loss: 0.077\n",
            "[24, 12000] loss: 0.068\n",
            "tensor(2.3685)\n",
            "[25,  2000] loss: 0.036\n",
            "[25,  4000] loss: 0.043\n",
            "[25,  6000] loss: 0.065\n",
            "[25,  8000] loss: 0.044\n",
            "[25, 10000] loss: 0.069\n",
            "[25, 12000] loss: 0.054\n",
            "tensor(3.2272e-08)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRTcPidM4XND"
      },
      "source": [
        "* https://discuss.pytorch.org/t/check-the-norm-of-gradients/27961 \r\n",
        "\r\n",
        "\r\n",
        "* https://discuss.pytorch.org/t/how-to-get-gradient-of-loss/16955 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIM3BL6T4XNE",
        "outputId": "a56699b0-de7f-4ba3-fe4d-3c190fa0cc43"
      },
      "source": [
        "correct = 0\r\n",
        "total = 0\r\n",
        "with torch.no_grad():\r\n",
        "    for data in testloader:\r\n",
        "        images, labels = data\r\n",
        "        outputs = net(images)\r\n",
        "        _, predicted = torch.max(outputs.data, 1)\r\n",
        "        total += labels.size(0)\r\n",
        "        correct += (predicted == labels).sum().item()\r\n",
        "\r\n",
        "print('Accuracy of the network on the 10000 test images: %f %%' % (\r\n",
        "    100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 74.550000 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brfwqrjJ4XNE",
        "outputId": "0f77983f-add0-49ad-8a03-f6dae7bee5de"
      },
      "source": [
        "class_correct = list(0. for i in range(10))\r\n",
        "class_total = list(0. for i in range(10))\r\n",
        "with torch.no_grad():\r\n",
        "    for data in testloader:\r\n",
        "        images, labels = data\r\n",
        "        outputs = net(images)\r\n",
        "        _, predicted = torch.max(outputs, 1)\r\n",
        "        c = (predicted == labels).squeeze()\r\n",
        "        for i in range(4):\r\n",
        "            label = labels[i]\r\n",
        "            class_correct[label] += c[i].item()\r\n",
        "            class_total[label] += 1\r\n",
        "\r\n",
        "\r\n",
        "for i in range(10):\r\n",
        "    print('Accuracy of %5s : %2d %%' % (\r\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of plane : 77 %\n",
            "Accuracy of   car : 81 %\n",
            "Accuracy of  bird : 61 %\n",
            "Accuracy of   cat : 65 %\n",
            "Accuracy of  deer : 74 %\n",
            "Accuracy of   dog : 61 %\n",
            "Accuracy of  frog : 81 %\n",
            "Accuracy of horse : 74 %\n",
            "Accuracy of  ship : 81 %\n",
            "Accuracy of truck : 86 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc3jeFnG4JHa"
      },
      "source": [
        "### W/ Batch norm\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gNvd0du4JHh"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "# This comes from https://github.com/mtrencseni/pytorch-playground/blob/master/05-cifar-10/CIFAR-10.ipynb\r\n",
        "class Net(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Net, self).__init__()\r\n",
        "        self.conv1 = nn.Conv2d(3,   64,  3)\r\n",
        "        self.conv1.add_module(\"BN1\", nn.BatchNorm2d(num_features=64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\r\n",
        "        self.conv2 = nn.Conv2d(64,  128, 3)\r\n",
        "        self.conv2.add_module(\"BN2\", nn.BatchNorm2d(num_features=128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\r\n",
        "        self.conv3 = nn.Conv2d(128, 256, 3)\r\n",
        "        self.conv3.add_module(\"BN3\", nn.BatchNorm2d(num_features=256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\r\n",
        "        self.pool = nn.MaxPool2d(2, 2)\r\n",
        "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\r\n",
        "        self.fc2 = nn.Linear(128, 256)\r\n",
        "        self.fc3 = nn.Linear(256, 10)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.pool(F.relu(self.conv1(x)))\r\n",
        "        x = self.pool(F.relu(self.conv2(x)))\r\n",
        "        x = self.pool(F.relu(self.conv3(x)))\r\n",
        "        x = x.view(-1, 64 * 4 * 4)\r\n",
        "        x = F.relu(self.fc1(x))\r\n",
        "        x = F.relu(self.fc2(x))\r\n",
        "        x = self.fc3(x)\r\n",
        "        return F.log_softmax(x, dim=1)\r\n",
        "\r\n",
        "net = Net()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvJZGyAV4JHi"
      },
      "source": [
        "import torch.optim as optim\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32Oj2uXC4JHi"
      },
      "source": [
        "Soruces:\r\n",
        "* https://discuss.pytorch.org/t/how-are-optimizer-step-and-loss-backward-related/7350\r\n",
        "* https://discuss.pytorch.org/t/how-to-interpret-the-grad-tensor-in-the-optimizer/34892\r\n",
        "* https://discuss.pytorch.org/t/a-problem-about-optimizer-param-groups-in-step-function/14463/3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xAOp5HF-4JHi",
        "outputId": "c47ba998-a8cc-474b-93c6-de26951fcce4"
      },
      "source": [
        "x_vals = range(1,26)\r\n",
        "l2_norms = []\r\n",
        "\r\n",
        "for epoch in range(25):  # loop over the dataset multiple times\r\n",
        "\r\n",
        "    running_loss = 0.0\r\n",
        "    for i, data in enumerate(trainloader, 0):\r\n",
        "        # get the inputs\r\n",
        "        inputs, labels = data\r\n",
        "\r\n",
        "        # zero the parameter gradients\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        # forward + backward + optimize\r\n",
        "        outputs = net(inputs)\r\n",
        "        loss = criterion(outputs, labels)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        # print statistics\r\n",
        "        running_loss += loss.item()\r\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\r\n",
        "            print('[%d, %5d] loss: %.3f' %\r\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\r\n",
        "            running_loss = 0.0\r\n",
        "\r\n",
        "    #I think this is computing the right l2 norm:\r\n",
        "    #l2_norm = torch.sum(torch.square(optimizer.param_groups[0]['params'][0].grad)).item()\r\n",
        "    #print(l2_norm)\r\n",
        "    #l2_norms.append(l2_norm)\r\n",
        "    \r\n",
        "    grad_norm_sum = 0\r\n",
        "    for p in net.parameters():\r\n",
        "      if p.grad != None:\r\n",
        "        grad_norm_sum += torch.sum(torch.square(p.grad))\r\n",
        "    l2_norms.append(grad_norm_sum)\r\n",
        "    print(grad_norm_sum)\r\n",
        "    # THIS IS IMPORTANT: https://discuss.pytorch.org/t/difference-between-gradients-from-network-parameters-and-register-backward-hook/4580/3\r\n",
        "    # This is also important: https://discuss.pytorch.org/t/get-the-gradient-of-the-network-parameters/50575\r\n",
        "    \r\n",
        "plt.scatter(x_vals, l2_norms)\r\n",
        "plt.show()\r\n",
        "\r\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  2000] loss: 2.231\n",
            "[1,  4000] loss: 1.898\n",
            "[1,  6000] loss: 1.693\n",
            "[1,  8000] loss: 1.552\n",
            "[1, 10000] loss: 1.451\n",
            "[1, 12000] loss: 1.382\n",
            "tensor(22.5073)\n",
            "[2,  2000] loss: 1.269\n",
            "[2,  4000] loss: 1.232\n",
            "[2,  6000] loss: 1.204\n",
            "[2,  8000] loss: 1.147\n",
            "[2, 10000] loss: 1.106\n",
            "[2, 12000] loss: 1.083\n",
            "tensor(51.2795)\n",
            "[3,  2000] loss: 0.951\n",
            "[3,  4000] loss: 0.958\n",
            "[3,  6000] loss: 0.922\n",
            "[3,  8000] loss: 0.911\n",
            "[3, 10000] loss: 0.889\n",
            "[3, 12000] loss: 0.872\n",
            "tensor(34.7096)\n",
            "[4,  2000] loss: 0.757\n",
            "[4,  4000] loss: 0.762\n",
            "[4,  6000] loss: 0.767\n",
            "[4,  8000] loss: 0.756\n",
            "[4, 10000] loss: 0.733\n",
            "[4, 12000] loss: 0.723\n",
            "tensor(70.2413)\n",
            "[5,  2000] loss: 0.605\n",
            "[5,  4000] loss: 0.597\n",
            "[5,  6000] loss: 0.624\n",
            "[5,  8000] loss: 0.613\n",
            "[5, 10000] loss: 0.631\n",
            "[5, 12000] loss: 0.644\n",
            "tensor(50.7237)\n",
            "[6,  2000] loss: 0.471\n",
            "[6,  4000] loss: 0.515\n",
            "[6,  6000] loss: 0.521\n",
            "[6,  8000] loss: 0.541\n",
            "[6, 10000] loss: 0.508\n",
            "[6, 12000] loss: 0.537\n",
            "tensor(43.9823)\n",
            "[7,  2000] loss: 0.382\n",
            "[7,  4000] loss: 0.405\n",
            "[7,  6000] loss: 0.423\n",
            "[7,  8000] loss: 0.450\n",
            "[7, 10000] loss: 0.455\n",
            "[7, 12000] loss: 0.470\n",
            "tensor(68.9258)\n",
            "[8,  2000] loss: 0.293\n",
            "[8,  4000] loss: 0.326\n",
            "[8,  6000] loss: 0.360\n",
            "[8,  8000] loss: 0.356\n",
            "[8, 10000] loss: 0.385\n",
            "[8, 12000] loss: 0.392\n",
            "tensor(0.2006)\n",
            "[9,  2000] loss: 0.241\n",
            "[9,  4000] loss: 0.279\n",
            "[9,  6000] loss: 0.295\n",
            "[9,  8000] loss: 0.324\n",
            "[9, 10000] loss: 0.328\n",
            "[9, 12000] loss: 0.337\n",
            "tensor(196.9998)\n",
            "[10,  2000] loss: 0.202\n",
            "[10,  4000] loss: 0.227\n",
            "[10,  6000] loss: 0.243\n",
            "[10,  8000] loss: 0.273\n",
            "[10, 10000] loss: 0.258\n",
            "[10, 12000] loss: 0.276\n",
            "tensor(577.6006)\n",
            "[11,  2000] loss: 0.152\n",
            "[11,  4000] loss: 0.178\n",
            "[11,  6000] loss: 0.200\n",
            "[11,  8000] loss: 0.220\n",
            "[11, 10000] loss: 0.230\n",
            "[11, 12000] loss: 0.253\n",
            "tensor(0.1598)\n",
            "[12,  2000] loss: 0.140\n",
            "[12,  4000] loss: 0.165\n",
            "[12,  6000] loss: 0.179\n",
            "[12,  8000] loss: 0.196\n",
            "[12, 10000] loss: 0.200\n",
            "[12, 12000] loss: 0.215\n",
            "tensor(535.1991)\n",
            "[13,  2000] loss: 0.126\n",
            "[13,  4000] loss: 0.131\n",
            "[13,  6000] loss: 0.144\n",
            "[13,  8000] loss: 0.172\n",
            "[13, 10000] loss: 0.163\n",
            "[13, 12000] loss: 0.189\n",
            "tensor(0.0028)\n",
            "[14,  2000] loss: 0.100\n",
            "[14,  4000] loss: 0.124\n",
            "[14,  6000] loss: 0.140\n",
            "[14,  8000] loss: 0.129\n",
            "[14, 10000] loss: 0.162\n",
            "[14, 12000] loss: 0.167\n",
            "tensor(21.8389)\n",
            "[15,  2000] loss: 0.093\n",
            "[15,  4000] loss: 0.102\n",
            "[15,  6000] loss: 0.106\n",
            "[15,  8000] loss: 0.135\n",
            "[15, 10000] loss: 0.132\n",
            "[15, 12000] loss: 0.148\n",
            "tensor(4252.3960)\n",
            "[16,  2000] loss: 0.084\n",
            "[16,  4000] loss: 0.081\n",
            "[16,  6000] loss: 0.129\n",
            "[16,  8000] loss: 0.121\n",
            "[16, 10000] loss: 0.126\n",
            "[16, 12000] loss: 0.122\n",
            "tensor(0.0243)\n",
            "[17,  2000] loss: 0.057\n",
            "[17,  4000] loss: 0.073\n",
            "[17,  6000] loss: 0.106\n",
            "[17,  8000] loss: 0.122\n",
            "[17, 10000] loss: 0.119\n",
            "[17, 12000] loss: 0.113\n",
            "tensor(338.0668)\n",
            "[18,  2000] loss: 0.073\n",
            "[18,  4000] loss: 0.084\n",
            "[18,  6000] loss: 0.095\n",
            "[18,  8000] loss: 0.087\n",
            "[18, 10000] loss: 0.090\n",
            "[18, 12000] loss: 0.094\n",
            "tensor(0.0007)\n",
            "[19,  2000] loss: 0.062\n",
            "[19,  4000] loss: 0.066\n",
            "[19,  6000] loss: 0.089\n",
            "[19,  8000] loss: 0.099\n",
            "[19, 10000] loss: 0.077\n",
            "[19, 12000] loss: 0.103\n",
            "tensor(2.2072)\n",
            "[20,  2000] loss: 0.058\n",
            "[20,  4000] loss: 0.071\n",
            "[20,  6000] loss: 0.082\n",
            "[20,  8000] loss: 0.069\n",
            "[20, 10000] loss: 0.077\n",
            "[20, 12000] loss: 0.093\n",
            "tensor(810.6954)\n",
            "[21,  2000] loss: 0.076\n",
            "[21,  4000] loss: 0.059\n",
            "[21,  6000] loss: 0.061\n",
            "[21,  8000] loss: 0.099\n",
            "[21, 10000] loss: 0.078\n",
            "[21, 12000] loss: 0.097\n",
            "tensor(0.0050)\n",
            "[22,  2000] loss: 0.044\n",
            "[22,  4000] loss: 0.056\n",
            "[22,  6000] loss: 0.061\n",
            "[22,  8000] loss: 0.068\n",
            "[22, 10000] loss: 0.080\n",
            "[22, 12000] loss: 0.075\n",
            "tensor(137.1410)\n",
            "[23,  2000] loss: 0.040\n",
            "[23,  4000] loss: 0.042\n",
            "[23,  6000] loss: 0.037\n",
            "[23,  8000] loss: 0.062\n",
            "[23, 10000] loss: 0.071\n",
            "[23, 12000] loss: 0.064\n",
            "tensor(0.0149)\n",
            "[24,  2000] loss: 0.052\n",
            "[24,  4000] loss: 0.052\n",
            "[24,  6000] loss: 0.046\n",
            "[24,  8000] loss: 0.066\n",
            "[24, 10000] loss: 0.050\n",
            "[24, 12000] loss: 0.062\n",
            "tensor(0.0716)\n",
            "[25,  2000] loss: 0.037\n",
            "[25,  4000] loss: 0.038\n",
            "[25,  6000] loss: 0.049\n",
            "[25,  8000] loss: 0.062\n",
            "[25, 10000] loss: 0.056\n",
            "[25, 12000] loss: 0.070\n",
            "tensor(147.0275)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3_ghpue4JHj"
      },
      "source": [
        "* https://discuss.pytorch.org/t/check-the-norm-of-gradients/27961 \r\n",
        "\r\n",
        "\r\n",
        "* https://discuss.pytorch.org/t/how-to-get-gradient-of-loss/16955 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwuEXUUm4JHk",
        "outputId": "37e41ce6-f46a-467f-9120-5566f588eba6"
      },
      "source": [
        "correct = 0\r\n",
        "total = 0\r\n",
        "with torch.no_grad():\r\n",
        "    for data in testloader:\r\n",
        "        images, labels = data\r\n",
        "        outputs = net(images)\r\n",
        "        _, predicted = torch.max(outputs.data, 1)\r\n",
        "        total += labels.size(0)\r\n",
        "        correct += (predicted == labels).sum().item()\r\n",
        "\r\n",
        "print('Accuracy of the network on the 10000 test images: %f %%' % (\r\n",
        "    100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 75.510000 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yebYtmm4JHk",
        "outputId": "e5a83968-0e7a-4a15-da1b-323c0feb7f4e"
      },
      "source": [
        "class_correct = list(0. for i in range(10))\r\n",
        "class_total = list(0. for i in range(10))\r\n",
        "with torch.no_grad():\r\n",
        "    for data in testloader:\r\n",
        "        images, labels = data\r\n",
        "        outputs = net(images)\r\n",
        "        _, predicted = torch.max(outputs, 1)\r\n",
        "        c = (predicted == labels).squeeze()\r\n",
        "        for i in range(4):\r\n",
        "            label = labels[i]\r\n",
        "            class_correct[label] += c[i].item()\r\n",
        "            class_total[label] += 1\r\n",
        "\r\n",
        "\r\n",
        "for i in range(10):\r\n",
        "    print('Accuracy of %5s : %2d %%' % (\r\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of plane : 82 %\n",
            "Accuracy of   car : 91 %\n",
            "Accuracy of  bird : 64 %\n",
            "Accuracy of   cat : 56 %\n",
            "Accuracy of  deer : 67 %\n",
            "Accuracy of   dog : 64 %\n",
            "Accuracy of  frog : 79 %\n",
            "Accuracy of horse : 84 %\n",
            "Accuracy of  ship : 82 %\n",
            "Accuracy of truck : 81 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8t21JGZyUr-"
      },
      "source": [
        "# 2. Residual connections (2 points)\n",
        "\n",
        "Please make use of the model code from Homework 4 question 2 as you work on this question.\n",
        "\n",
        "If your model didn't use residual connections, add them. If it already had residual connections, remove them. How does the performance change? Please experiment with applying residual connections around different blocks, where a block is a subset of layers (for example, add a residual connection around a single convolution/nonlinearity combination, or a residual connection around two convolution/nonlinearity combinations). Note that you may need to add 1x1 convolutions in the residual path if your block changes the shape (spatial or number of channels) of the input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQq2bxnDr03Q"
      },
      "source": [
        "\r\n",
        "\r\n",
        "*   https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/deep_residual_network/main.py \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK6b4fox-sak"
      },
      "source": [
        "### W/ RESIDUAL CONNECTIONS (my original network did not have residual connections)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_16NbJV-sat"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "# This comes from https://github.com/mtrencseni/pytorch-playground/blob/master/05-cifar-10/CIFAR-10.ipynb\r\n",
        "class Net(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Net, self).__init__()\r\n",
        "        self.conv1 = nn.Conv2d(3,   64,  3)\r\n",
        "        self.conv2 = nn.Conv2d(64,  128, 3)\r\n",
        "        self.conv3 = nn.Conv2d(128, 256, 3)\r\n",
        "\r\n",
        "        self.downsample = nn.Conv2d(3, 256, 1, 16)\r\n",
        "\r\n",
        "        self.pool = nn.MaxPool2d(2, 2)\r\n",
        "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\r\n",
        "        self.fc2 = nn.Linear(128, 256)\r\n",
        "        self.fc3 = nn.Linear(256, 10)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        residual = x\r\n",
        "        \r\n",
        "        x = self.pool(F.relu(self.conv1(x)))\r\n",
        "        x = self.pool(F.relu(self.conv2(x)))\r\n",
        "        x = self.pool(F.relu(self.conv3(x)))\r\n",
        "        \r\n",
        "        residual = self.downsample(residual)\r\n",
        "        #print(\"x.shape\")\r\n",
        "        #print(x.shape)\r\n",
        "        #print(\"residual.shape\")\r\n",
        "        #print(residual.shape)\r\n",
        "\r\n",
        "        x += residual\r\n",
        "\r\n",
        "        x = x.view(-1, 64 * 4 * 4)\r\n",
        "        x = F.relu(self.fc1(x))\r\n",
        "        x = F.relu(self.fc2(x))\r\n",
        "        x = self.fc3(x)\r\n",
        "        return F.log_softmax(x, dim=1)\r\n",
        "\r\n",
        "net = Net()"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCnhsJ5r-sau"
      },
      "source": [
        "import torch.optim as optim\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTEo0zA--sau"
      },
      "source": [
        "Soruces:\r\n",
        "* https://discuss.pytorch.org/t/how-are-optimizer-step-and-loss-backward-related/7350\r\n",
        "* https://discuss.pytorch.org/t/how-to-interpret-the-grad-tensor-in-the-optimizer/34892\r\n",
        "* https://discuss.pytorch.org/t/a-problem-about-optimizer-param-groups-in-step-function/14463/3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bD09b_6u-sau",
        "outputId": "be00aa88-5600-497e-c831-d5eaae0d4083"
      },
      "source": [
        "x_vals = range(1,26)\r\n",
        "l2_norms = []\r\n",
        "\r\n",
        "for epoch in range(25):  # loop over the dataset multiple times\r\n",
        "\r\n",
        "    running_loss = 0.0\r\n",
        "    for i, data in enumerate(trainloader, 0):\r\n",
        "        # get the inputs\r\n",
        "        inputs, labels = data\r\n",
        "\r\n",
        "        # zero the parameter gradients\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        # forward + backward + optimize\r\n",
        "        outputs = net(inputs)\r\n",
        "        loss = criterion(outputs, labels)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        # print statistics\r\n",
        "        running_loss += loss.item()\r\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\r\n",
        "            print('[%d, %5d] loss: %.3f' %\r\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\r\n",
        "            running_loss = 0.0\r\n",
        "\r\n",
        "    #I think this is computing the right l2 norm:\r\n",
        "    #l2_norm = torch.sum(torch.square(optimizer.param_groups[0]['params'][0].grad)).item()\r\n",
        "    #print(l2_norm)\r\n",
        "    #l2_norms.append(l2_norm)\r\n",
        "    \r\n",
        "    grad_norm_sum = 0\r\n",
        "    for p in net.parameters():\r\n",
        "      if p.grad != None:\r\n",
        "        grad_norm_sum += torch.sum(torch.square(p.grad))\r\n",
        "    l2_norms.append(grad_norm_sum)\r\n",
        "    print(grad_norm_sum)\r\n",
        "    # THIS IS IMPORTANT: https://discuss.pytorch.org/t/difference-between-gradients-from-network-parameters-and-register-backward-hook/4580/3\r\n",
        "    # This is also important: https://discuss.pytorch.org/t/get-the-gradient-of-the-network-parameters/50575\r\n",
        "    \r\n",
        "plt.scatter(x_vals, l2_norms)\r\n",
        "plt.show()\r\n",
        "\r\n",
        "print('Finished Training')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  2000] loss: 2.090\n",
            "[1,  4000] loss: 1.793\n",
            "[1,  6000] loss: 1.646\n",
            "[1,  8000] loss: 1.556\n",
            "[1, 10000] loss: 1.460\n",
            "[1, 12000] loss: 1.398\n",
            "tensor(7.4496)\n",
            "[2,  2000] loss: 1.324\n",
            "[2,  4000] loss: 1.275\n",
            "[2,  6000] loss: 1.219\n",
            "[2,  8000] loss: 1.194\n",
            "[2, 10000] loss: 1.134\n",
            "[2, 12000] loss: 1.097\n",
            "tensor(73.2623)\n",
            "[3,  2000] loss: 1.033\n",
            "[3,  4000] loss: 1.012\n",
            "[3,  6000] loss: 0.984\n",
            "[3,  8000] loss: 0.961\n",
            "[3, 10000] loss: 0.951\n",
            "[3, 12000] loss: 0.923\n",
            "tensor(60.3675)\n",
            "[4,  2000] loss: 0.811\n",
            "[4,  4000] loss: 0.835\n",
            "[4,  6000] loss: 0.820\n",
            "[4,  8000] loss: 0.832\n",
            "[4, 10000] loss: 0.799\n",
            "[4, 12000] loss: 0.793\n",
            "tensor(4.7921)\n",
            "[5,  2000] loss: 0.677\n",
            "[5,  4000] loss: 0.689\n",
            "[5,  6000] loss: 0.697\n",
            "[5,  8000] loss: 0.709\n",
            "[5, 10000] loss: 0.705\n",
            "[5, 12000] loss: 0.683\n",
            "tensor(244.8655)\n",
            "[6,  2000] loss: 0.549\n",
            "[6,  4000] loss: 0.593\n",
            "[6,  6000] loss: 0.596\n",
            "[6,  8000] loss: 0.593\n",
            "[6, 10000] loss: 0.619\n",
            "[6, 12000] loss: 0.597\n",
            "tensor(2.5447)\n",
            "[7,  2000] loss: 0.461\n",
            "[7,  4000] loss: 0.484\n",
            "[7,  6000] loss: 0.502\n",
            "[7,  8000] loss: 0.501\n",
            "[7, 10000] loss: 0.524\n",
            "[7, 12000] loss: 0.529\n",
            "tensor(2.1634)\n",
            "[8,  2000] loss: 0.374\n",
            "[8,  4000] loss: 0.388\n",
            "[8,  6000] loss: 0.433\n",
            "[8,  8000] loss: 0.431\n",
            "[8, 10000] loss: 0.444\n",
            "[8, 12000] loss: 0.437\n",
            "tensor(487.6230)\n",
            "[9,  2000] loss: 0.303\n",
            "[9,  4000] loss: 0.319\n",
            "[9,  6000] loss: 0.347\n",
            "[9,  8000] loss: 0.378\n",
            "[9, 10000] loss: 0.370\n",
            "[9, 12000] loss: 0.388\n",
            "tensor(0.5149)\n",
            "[10,  2000] loss: 0.246\n",
            "[10,  4000] loss: 0.288\n",
            "[10,  6000] loss: 0.287\n",
            "[10,  8000] loss: 0.302\n",
            "[10, 10000] loss: 0.324\n",
            "[10, 12000] loss: 0.344\n",
            "tensor(18.4906)\n",
            "[11,  2000] loss: 0.192\n",
            "[11,  4000] loss: 0.240\n",
            "[11,  6000] loss: 0.265\n",
            "[11,  8000] loss: 0.264\n",
            "[11, 10000] loss: 0.289\n",
            "[11, 12000] loss: 0.268\n",
            "tensor(2.9122)\n",
            "[12,  2000] loss: 0.168\n",
            "[12,  4000] loss: 0.188\n",
            "[12,  6000] loss: 0.220\n",
            "[12,  8000] loss: 0.254\n",
            "[12, 10000] loss: 0.237\n",
            "[12, 12000] loss: 0.267\n",
            "tensor(315.2409)\n",
            "[13,  2000] loss: 0.145\n",
            "[13,  4000] loss: 0.151\n",
            "[13,  6000] loss: 0.200\n",
            "[13,  8000] loss: 0.194\n",
            "[13, 10000] loss: 0.217\n",
            "[13, 12000] loss: 0.235\n",
            "tensor(0.0208)\n",
            "[14,  2000] loss: 0.124\n",
            "[14,  4000] loss: 0.136\n",
            "[14,  6000] loss: 0.170\n",
            "[14,  8000] loss: 0.173\n",
            "[14, 10000] loss: 0.186\n",
            "[14, 12000] loss: 0.195\n",
            "tensor(0.1155)\n",
            "[15,  2000] loss: 0.110\n",
            "[15,  4000] loss: 0.117\n",
            "[15,  6000] loss: 0.203\n",
            "[15,  8000] loss: 0.154\n",
            "[15, 10000] loss: 0.174\n",
            "[15, 12000] loss: 0.191\n",
            "tensor(9.4397)\n",
            "[16,  2000] loss: 0.117\n",
            "[16,  4000] loss: 0.118\n",
            "[16,  6000] loss: 0.120\n",
            "[16,  8000] loss: 0.144\n",
            "[16, 10000] loss: 0.155\n",
            "[16, 12000] loss: 0.164\n",
            "tensor(0.6830)\n",
            "[17,  2000] loss: 0.096\n",
            "[17,  4000] loss: 0.110\n",
            "[17,  6000] loss: 0.103\n",
            "[17,  8000] loss: 0.107\n",
            "[17, 10000] loss: 0.143\n",
            "[17, 12000] loss: 0.146\n",
            "tensor(1450.4799)\n",
            "[18,  2000] loss: 0.090\n",
            "[18,  4000] loss: 0.098\n",
            "[18,  6000] loss: 0.115\n",
            "[18,  8000] loss: 0.103\n",
            "[18, 10000] loss: 0.143\n",
            "[18, 12000] loss: 0.124\n",
            "tensor(0.0323)\n",
            "[19,  2000] loss: 0.064\n",
            "[19,  4000] loss: 0.080\n",
            "[19,  6000] loss: 0.094\n",
            "[19,  8000] loss: 0.110\n",
            "[19, 10000] loss: 0.103\n",
            "[19, 12000] loss: 0.127\n",
            "tensor(0.0766)\n",
            "[20,  2000] loss: 0.074\n",
            "[20,  4000] loss: 0.083\n",
            "[20,  6000] loss: 0.096\n",
            "[20,  8000] loss: 0.098\n",
            "[20, 10000] loss: 0.111\n",
            "[20, 12000] loss: 0.124\n",
            "tensor(768.8096)\n",
            "[21,  2000] loss: 0.058\n",
            "[21,  4000] loss: 0.080\n",
            "[21,  6000] loss: 0.093\n",
            "[21,  8000] loss: 0.092\n",
            "[21, 10000] loss: 0.102\n",
            "[21, 12000] loss: 0.128\n",
            "tensor(12.2814)\n",
            "[22,  2000] loss: 0.070\n",
            "[22,  4000] loss: 0.077\n",
            "[22,  6000] loss: 0.107\n",
            "[22,  8000] loss: 0.086\n",
            "[22, 10000] loss: 0.122\n",
            "[22, 12000] loss: 0.106\n",
            "tensor(25.3405)\n",
            "[23,  2000] loss: 0.055\n",
            "[23,  4000] loss: 0.095\n",
            "[23,  6000] loss: 0.107\n",
            "[23,  8000] loss: 0.084\n",
            "[23, 10000] loss: 0.091\n",
            "[23, 12000] loss: 0.104\n",
            "tensor(0.0290)\n",
            "[24,  2000] loss: 0.062\n",
            "[24,  4000] loss: 0.075\n",
            "[24,  6000] loss: 0.089\n",
            "[24,  8000] loss: 0.089\n",
            "[24, 10000] loss: 0.083\n",
            "[24, 12000] loss: 0.091\n",
            "tensor(2.2840e-07)\n",
            "[25,  2000] loss: 0.073\n",
            "[25,  4000] loss: 0.079\n",
            "[25,  6000] loss: 0.071\n",
            "[25,  8000] loss: 0.082\n",
            "[25, 10000] loss: 0.078\n",
            "[25, 12000] loss: 0.087\n",
            "tensor(1.2042e-09)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAVE0lEQVR4nO3df4wc5X3H8fenZ4dcfjRn4Oris6ndxLqKQlujFVAlihC0nCFRfEUJglbFpJbcqqRNSuRgp1WJklaQOg0hUorkBBojURJKHGNVtBeLENFGhXLGBPMjF04EYq8NvsQcScqlMc63f+xzsD7ubHZ2d269z+clnXb2mWd2nmfn7rOzz8zcKCIwM7M8/NJ8N8DMzMrj0Dczy4hD38wsIw59M7OMOPTNzDKyYL4bcCynnnpqLF++fL6bYWZ2Qtm1a9cPI6J/tnkdHfrLly9ndHR0vpthZnZCkfTsXPM8vGNmlhGHvplZRhz6ZmYZceibmWXEoW9mlpGOPnvHzNpr++4qm0fG2D85xZK+XjYMDTK8amC+m2Vt5NA3y9T23VU2bdvD1OEjAFQnp9i0bQ+Ag7+LeXjHLFObR8ZeCfxpU4ePsHlkbJ5aZGVw6Jtlav/kVEPl1h0c+maZWtLX21C5dQeHvlmmNgwN0ruw56iy3oU9bBganKcWWRl8INcsU9MHa332Tl6OG/qSbgXeCxyMiDNnzPso8BmgPyJ+KEnATcAlwEvAVRHxcKq7FvibtOjfRcTW1nXDzIoYXjXgkM/M6xne+TKwemahpGXARcAP6oovBlamn/XAzanuycB1wLnAOcB1khY103AzM2vccUM/Iu4HDs0y60bgY0DUla0BbouaB4A+SacBQ8DOiDgUES8AO5nlg8TMzNqr0IFcSWuAakR8Z8asAWBv3fN9qWyu8tlee72kUUmjExMTRZpnZmZzaDj0Jb0J+Djwt61vDkTEloioRESlv3/WG7+YmVlBRfb03w6sAL4j6RlgKfCwpF8FqsCyurpLU9lc5WZmVqKGQz8i9kTEr0TE8ohYTm2o5uyIeA7YAVypmvOAFyPiADACXCRpUTqAe1EqMzOzEh039CXdAfw3MChpn6R1x6h+D/A0MA58EfhzgIg4BHwKeCj9fDKVmZlZiRQRx681TyqVSvjG6GZmjZG0KyIqs83zv2EwM8uIQ9/MLCMOfTOzjDj0zcwy4tA3M8uIQ9/MLCMOfTOzjDj0zcwy4tA3M8uIQ9/MLCMOfTOzjDj0zcwy4tA3M8uIQ9/MLCMOfTOzjDj0zcwy4tA3M8uIQ9/MLCOv5x65t0o6KOmxurLNkr4r6VFJX5fUVzdvk6RxSWOShurKV6eycUkbW98VMzM7ntezp/9lYPWMsp3AmRHxW8D3gE0Aks4ALgd+My3zT5J6JPUAXwAuBs4Arkh1zcysRMcN/Yi4Hzg0o+wbEfFyevoAsDRNrwG+EhH/FxHfB8aBc9LPeEQ8HRE/B76S6pqZWYlaMab/J8C/p+kBYG/dvH2pbK7y15C0XtKopNGJiYkWNM/MzKY1FfqS/hp4Gbi9Nc2BiNgSEZWIqPT397fqZc3MDFhQdEFJVwHvBS6MiEjFVWBZXbWlqYxjlJuZWUkK7elLWg18DHhfRLxUN2sHcLmkkyStAFYC/wM8BKyUtELSG6gd7N3RXNPNzKxRx93Tl3QHcD5wqqR9wHXUztY5CdgpCeCBiPiziHhc0p3AE9SGfa6OiCPpdT4EjAA9wK0R8Xgb+mNmZsegV0dmOk+lUonR0dH5boaZ2QlF0q6IqMw2z1fkmpllxKFvZpYRh76ZWUYc+mZmGXHom5llxKFvZpYRh76ZWUYc+mZmGXHom5llxKFvZpYRh76ZWUYc+mZmGXHom5llxKFvZpYRh76ZWUYc+mZmGXHom5llxKFvZpaR44a+pFslHZT0WF3ZyZJ2SnoqPS5K5ZL0eUnjkh6VdHbdMmtT/ackrW1Pd8zM7Fhez57+l4HVM8o2AvdGxErg3vQc4GJgZfpZD9wMtQ8JajdUPxc4B7hu+oPCzMzKc9zQj4j7gUMzitcAW9P0VmC4rvy2qHkA6JN0GjAE7IyIQxHxArCT136QmJlZmxUd018cEQfS9HPA4jQ9AOytq7cvlc1VbmZmJWr6QG5EBBAtaAsAktZLGpU0OjEx0aqXNTMziof+82nYhvR4MJVXgWV19ZamsrnKXyMitkREJSIq/f39BZtnZmazKRr6O4DpM3DWAnfXlV+ZzuI5D3gxDQONABdJWpQO4F6UyszMrEQLjldB0h3A+cCpkvZROwvnBuBOSeuAZ4HLUvV7gEuAceAl4IMAEXFI0qeAh1K9T0bEzIPDZmbWZqoNyXemSqUSo6Oj890MM7MTiqRdEVGZbZ6vyDUzy4hD38wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMNBX6kv5K0uOSHpN0h6Q3Sloh6UFJ45K+KukNqe5J6fl4mr+8FR0wM7PXr3DoSxoA/hKoRMSZQA9wOfBp4MaIeAfwArAuLbIOeCGV35jqmZlZiZod3lkA9EpaALwJOABcANyV5m8FhtP0mvScNP9CSWpy/WZm1oDCoR8RVeAzwA+ohf2LwC5gMiJeTtX2AQNpegDYm5Z9OdU/ZebrSlovaVTS6MTERNHmmZnZLBYUXVDSImp77yuASeBfgdXNNigitgBbACqVSjT7emZ24tm+u8rmkTH2T06xpK+XDUODDK8aOP6CdlyFQx/4PeD7ETEBIGkb8E6gT9KCtDe/FKim+lVgGbAvDQe9DfhRE+s3sy60fXeVTdv2MHX4CADVySk2bdsD4OBvgWbG9H8AnCfpTWls/kLgCeA+4P2pzlrg7jS9Iz0nzf9mRHhP3syOsnlk7JXAnzZ1+AibR8bmqUXdpZkx/QepHZB9GNiTXmsLcC1wjaRxamP2t6RFbgFOSeXXABubaLeZdan9k1MNlVtjmhneISKuA66bUfw0cM4sdX8GfKCZ9ZlZ91vS10t1loBf0tc7D63pPr4i18w6yoahQXoX9hxV1ruwhw1Dg/PUou7S1J6+mVmrTR+s9dk77eHQN7OOM7xqwCHfJh7eMTPLiEPfzCwjDn0zs4w49M3MMuLQNzPLiEPfzCwjDn0zs4w49M3MMuLQNzPLiEPfzCwjDn0zs4w49M3MMuLQNzPLiEPfzCwjDn0zs4w0FfqS+iTdJem7kp6U9LuSTpa0U9JT6XFRqitJn5c0LulRSWe3pgtmZvZ6NbunfxPwHxHxG8BvA09Su+H5vRGxEriXV2+AfjGwMv2sB25uct1mZtagwqEv6W3Au4FbACLi5xExCawBtqZqW4HhNL0GuC1qHgD6JJ1WuOVmZtawZvb0VwATwD9L2i3pS5LeDCyOiAOpznPA4jQ9AOytW35fKjuKpPWSRiWNTkxMNNE8MzObqZnQXwCcDdwcEauA/+XVoRwAIiKAaORFI2JLRFQiotLf399E88zMbKZmQn8fsC8iHkzP76L2IfD89LBNejyY5leBZXXLL01lZmZWksKhHxHPAXslDaaiC4EngB3A2lS2Frg7Te8Arkxn8ZwHvFg3DGRmZiVY0OTyfwHcLukNwNPAB6l9kNwpaR3wLHBZqnsPcAkwDryU6pqZWYmaCv2IeASozDLrwlnqBnB1M+szM7Pm+IpcM7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLS7Hn6lqHtu6tsHhlj/+QUS/p62TA0yPCq1/wbJTPrQA59a8j23VU2bdvD1OEjAFQnp9i0bQ+Ag9/sBODhHWvI5pGxVwJ/2tThI2weGZunFplZIxz61pD9k1MNlZtZZ3HoW0OW9PU2VG5mncWhbw3ZMDRI78Keo8p6F/awYWhwjiXMrJP4QK41ZPpgrc/eMTsxOfStYcOrBhzyZicoD++YmWXEoW9mlhGHvplZRhz6ZmYZaTr0JfVI2i3p39LzFZIelDQu6avp/rlIOik9H0/zlze7bjMza0wr9vQ/DDxZ9/zTwI0R8Q7gBWBdKl8HvJDKb0z1zMysRE2FvqSlwHuAL6XnAi4A7kpVtgLDaXpNek6af2Gqb2ZmJWl2T/9zwMeAX6TnpwCTEfFyer4PmD6hewDYC5Dmv5jqH0XSekmjkkYnJiaabJ6ZmdUrHPqS3gscjIhdLWwPEbElIioRUenv72/lS5uZZa+ZK3LfCbxP0iXAG4FfBm4C+iQtSHvzS4Fqql8FlgH7JC0A3gb8qIn1m5lZgwrv6UfEpohYGhHLgcuBb0bEHwH3Ae9P1dYCd6fpHek5af43IyKKrt/MzBrXjvP0rwWukTRObcz+llR+C3BKKr8G2NiGdZuZ2TG05B+uRcS3gG+l6aeBc2ap8zPgA61Yn5mZFeMrcs3MMuLQNzPLiEPfzCwjvomKZW377qrvAmZZcehbtrbvrrJp2x6mDh8BoDo5xaZtewAc/Na1PLxj2do8MvZK4E+bOnyEzSNj89Qis/Zz6Fu29k9ONVRu1g0c+patJX29DZWbdQOHvmVrw9AgvQt7jirrXdjDhqHBeWqRWfv5QK5la/pgrc/esZw49C1rw6sGHPKWFQ/vmJllxKFvZpYRD+90MF8tamat5tDvUL5a1MzawcM7HcpXi5pZOzj0O5SvFjWzdnDodyhfLWpm7eDQ71C+WtTM2qFw6EtaJuk+SU9IelzSh1P5yZJ2SnoqPS5K5ZL0eUnjkh6VdHarOtGNhlcNcP2lZzHQ14uAgb5err/0LB/ENbOmNHP2zsvARyPiYUlvBXZJ2glcBdwbETdI2ghsBK4FLgZWpp9zgZvTo83BV4uaWasV3tOPiAMR8XCa/gnwJDAArAG2pmpbgeE0vQa4LWoeAPoknVa45WZm1rCWjOlLWg6sAh4EFkfEgTTrOWBxmh4A9tYtti+VzXyt9ZJGJY1OTEy0onlmZpY0HfqS3gJ8DfhIRPy4fl5EBBCNvF5EbImISkRU+vv7m22emZnVaSr0JS2kFvi3R8S2VPz89LBNejyYyqvAsrrFl6YyMzMrSTNn7wi4BXgyIj5bN2sHsDZNrwXuriu/Mp3Fcx7wYt0wkJmZlaCZs3feCfwxsEfSI6ns48ANwJ2S1gHPApelefcAlwDjwEvAB5tYt5mZFVA49CPivwDNMfvCWeoHcHXR9ZmZWfN8Ra6ZWUYc+mZmGXHom5llxKFvZpYRh76ZWUYc+mZmGXHom5llxKFvZpaRZq7I7Rrbd1fZPDLG/skplvT1smFo0P/H3sy6Uvahv313lU3b9jB1+AgA1ckpNm3bA+DgN7Ouk33obx4ZeyXwp00dPsLmkbFjhr6/HZjZiSj70N8/OdVQOfjbgZmduLI/kLukr7ehcjj2twMzs06WfehvGBqkd2HPUWW9C3vYMDQ45zJFvh2YmXWC7Id3podjGhmfX9LXS3WWgD/WtwMz6yy5HpfLPvShFvyNbOwNQ4NHjenD8b8dWONy/aO09sv5uJxDv4Ai3w6sMTn/UVrjGt1BKHrWXjfoytAvYw+x0W8HZbWrW+T8Rwn+XWlEkR2EnI/LlR76klYDNwE9wJci4oZWvn6n7iGW1a4iYVFGwDS6jqKn0pYRlO1+j4v+rnTidiyjXUV2EIoel2u0L534fpV69o6kHuALwMXAGcAVks5o5To69XTKMto1HRbVySmCV8Ni++5qS5cpo12NnkpbRj+KrqfRZYr8rnTqdiyjXUV2EIqctddoXzr1/Sr7lM1zgPGIeDoifg58BVjTyhV06te2MtpVJCzK+DAqso5G/yjL+rAv4z0u8rvSqduxjHYVudZmeNUA1196FgN9vQgY6Ovl+kvPKnwcoBX1iy7TqLKHdwaAvXXP9wHn1leQtB5YD3D66ac3vIJOPZ2yjHYVCYsyPoyKrKPRg+VlfdiX8R4X+V3p1O1YRruKnk3X6HG5RvvSqe9Xx12cFRFbIqISEZX+/v6Gly/yta0MZbSryB5PkWUaVXQdw6sG+PbGC/j+De/h2xsvOO61E0XW0agy3uMivyuduh3LaFeRvfYiGu1Lp75fZYd+FVhW93xpKmuZsn4BOrFdRcKijA+jbllH0fU0ukyR35VOfY/L2i6N7CAU1WhfOvX9UkS07MWOuzJpAfA94EJqYf8Q8IcR8fhs9SuVSoyOjpbWvm7QiWcLdNM6iq6nW/rfqX0vy4ly9o6kXRFRmXVemaGfGnMJ8Dlqp2zeGhF/P1ddh76ZWeOOFfqln6cfEfcA95S9XjMz68ADuWZm1j4OfTOzjDj0zcwy4tA3M8tI6WfvNELSBPBsenoq8MN5bM58yrnvkHf/c+475N3/Zvr+axEx69WtHR369SSNznUKUrfLue+Qd/9z7jvk3f929d3DO2ZmGXHom5ll5EQK/S3z3YB5lHPfIe/+59x3yLv/ben7CTOmb2ZmzTuR9vTNzKxJDn0zs4x0fOhLWi1pTNK4pI3z3Z6ySXpG0h5Jj0jq+n85KulWSQclPVZXdrKknZKeSo+L5rON7TJH3z8hqZq2/yPpv9R2HUnLJN0n6QlJj0v6cCrv+m1/jL63Zdt39Jh+upH694Dfp3ZrxYeAKyLiiXltWIkkPQNUIiKLC1QkvRv4KXBbRJyZyv4BOBQRN6QP/kURce18trMd5uj7J4CfRsRn5rNt7SbpNOC0iHhY0luBXcAwcBVdvu2P0ffLaMO27/Q9/bbfSN06S0TcDxyaUbwG2Jqmt1L7g+g6c/Q9CxFxICIeTtM/AZ6kdk/trt/2x+h7W3R66M92I/UT85Y7xQXwDUm70k3jc7Q4Ig6k6eeAxfPZmHnwIUmPpuGfrhvemEnScmAV8CCZbfsZfYc2bPtOD32Dd0XE2cDFwNVpCCBbURuP7Nwxyda7GXg78DvAAeAf57c57SXpLcDXgI9ExI/r53X7tp+l723Z9p0e+m2/kXqni4hqejwIfJ3akFdunk/jntPjnwfnuT2liYjnI+JIRPwC+CJdvP0lLaQWerdHxLZUnMW2n63v7dr2nR76DwErJa2Q9AbgcmDHPLepNJLenA7sIOnNwEXAY8deqivtANam6bXA3fPYllJNB17yB3Tp9pck4BbgyYj4bN2srt/2c/W9Xdu+o8/egcZupN5tJP06tb17qN3P+F+6vf+S7gDOp/ZvZZ8HrgO2A3cCp1P7V9uXRUTXHfCco+/nU/t6H8AzwJ/WjXF3DUnvAv4T2AP8IhV/nNrYdldv+2P0/QrasO07PvTNzKx1On14x8zMWsihb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlG/h/xTenFM5Zt4wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIZtNCZ9-sav"
      },
      "source": [
        "* https://discuss.pytorch.org/t/check-the-norm-of-gradients/27961 \r\n",
        "\r\n",
        "\r\n",
        "* https://discuss.pytorch.org/t/how-to-get-gradient-of-loss/16955 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAvpZP-V-sav",
        "outputId": "1a3bfa5e-debb-4b48-f8c8-54eccd786a16"
      },
      "source": [
        "correct = 0\r\n",
        "total = 0\r\n",
        "with torch.no_grad():\r\n",
        "    for data in testloader:\r\n",
        "        images, labels = data\r\n",
        "        outputs = net(images)\r\n",
        "        _, predicted = torch.max(outputs.data, 1)\r\n",
        "        total += labels.size(0)\r\n",
        "        correct += (predicted == labels).sum().item()\r\n",
        "\r\n",
        "print('Accuracy of the network on the 10000 test images: %f %%' % (\r\n",
        "    100 * correct / total))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 75.160000 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdcLbyjO-sav",
        "outputId": "f8ebf07c-97e3-47d0-cb10-0bec97e6e082"
      },
      "source": [
        "class_correct = list(0. for i in range(10))\r\n",
        "class_total = list(0. for i in range(10))\r\n",
        "with torch.no_grad():\r\n",
        "    for data in testloader:\r\n",
        "        images, labels = data\r\n",
        "        outputs = net(images)\r\n",
        "        _, predicted = torch.max(outputs, 1)\r\n",
        "        c = (predicted == labels).squeeze()\r\n",
        "        for i in range(4):\r\n",
        "            label = labels[i]\r\n",
        "            class_correct[label] += c[i].item()\r\n",
        "            class_total[label] += 1\r\n",
        "\r\n",
        "\r\n",
        "for i in range(10):\r\n",
        "    print('Accuracy of %5s : %2d %%' % (\r\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of plane : 81 %\n",
            "Accuracy of   car : 85 %\n",
            "Accuracy of  bird : 66 %\n",
            "Accuracy of   cat : 46 %\n",
            "Accuracy of  deer : 71 %\n",
            "Accuracy of   dog : 70 %\n",
            "Accuracy of  frog : 81 %\n",
            "Accuracy of horse : 79 %\n",
            "Accuracy of  ship : 85 %\n",
            "Accuracy of truck : 83 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUSQJKTNZZeh"
      },
      "source": [
        "## Relfections on Batch normalization\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "*   Batch norm made my accuracy go up, and throughout my tinkering (and reading ML literature), I found that it can generally support networks with a higher learning rate (for my specific implementation, I ended up using the same learning rate for both networks (with and without batch norm) \r\n",
        "*   Batch norm made my L2 norm of the gradients lower, and generally more consistant for my network.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJfe_XmDarKb"
      },
      "source": [
        "## Reflections on Residual Connections\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "*   Adding residual connections into my network made the accuracy go up. I also experimented with different resNet architectures (un-related to the network I built myself). What I noticed was that it trained much quicked, and achieved a much higher accuracy with fewer ephocs (One particular archintecture which I linked above as a source got me 80% accuracy in 10 ephocs, while the basic architecture I originally built without residual connections gets 75% after 25 ephocs)\r\n",
        "*   Residuals connections, like batch norm, made my L2 norm of the gradients lower and generally more consistant for my network as well.\r\n",
        "\r\n"
      ]
    }
  ]
}