{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Kaggle Housing Price Competition - HW 3",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CWjCzmfUp7F"
      },
      "source": [
        "# Homework 3\n",
        "\n",
        "*This homework was adapted from the STAT 157 course at Berkeley.*\n",
        "\n",
        "In this homework, we will build a model based real house sale data from a [Kaggle competition](https://www.kaggle.com/c/house-prices-advanced-regression-techniques). This notebook contains codes to download the dataset, build and train a baseline model, and save the results in the submission format. Your jobs are \n",
        "\n",
        "1. Develope a better model to reduce the prediction error. You can find some hints on the last section. \n",
        "\n",
        "2. Submit your results into Kaggle, take a sceenshot of your score, upload it to the image-sharing site of your choice, and embedding it within the notebook (you can use the markdown `![](https://colinraffel.com/images/me.jpg)`, replacing the URL with your uploaded screenshot).\n",
        "\n",
        "I recommend that you start as early as possible on this homework. Tuning hyper-parameters takes time, and Kaggle limits the number of submissions you can make per day.\n",
        "\n",
        "You can get a sense of how well you are doing by looking at your position on the Kaggle leaderboard.\n",
        "\n",
        "Finally, make sure you have a GPU runtime selected!\n",
        "\n",
        "## Accessing and Reading Data Sets\n",
        "\n",
        "The competition data is separated into training and test sets. Each record includes the property values of the house and attributes such as street type, year of construction, roof type, basement condition. The data includes multiple datatypes, including integers (year of construction), discrete labels (roof type), floating point numbers, etc.; Some data is missing and is thus labeled 'na'. The price of each house, namely the label, is only included in the training data set (it's a competition after all). The 'Data' tab on the competition tab has links to download the data.\n",
        "\n",
        "We will read and process the data using `pandas`, an [efficient data analysis toolkit](http://pandas.pydata.org/pandas-docs/stable/). Make sure you have `pandas` installed for the experiments in this section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-12T19:52:59.411749Z",
          "start_time": "2019-02-12T19:52:28.081528Z"
        },
        "attributes": {
          "classes": [],
          "id": "",
          "n": "3"
        },
        "scrolled": true,
        "id": "r79cz5KGUp7I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e46ea444-54f8-4874-ad12-d2c69d80a34d"
      },
      "source": [
        "!pip install d2l\n",
        "!pip install mxnet\n",
        "\n",
        "%matplotlib inline\n",
        "import d2l\n",
        "from mxnet import autograd, gluon, init, nd\n",
        "from mxnet.gluon import data as gdata, loss as gloss, nn, utils\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting d2l\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/2b/3515cd6f2898bf95306a5c58b065aeb045fdc25516f2b68b0f8409e320c3/d2l-0.16.1-py3-none-any.whl (76kB)\n",
            "\r\u001b[K     |████▎                           | 10kB 23.4MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20kB 30.2MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 30kB 22.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40kB 20.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 51kB 21.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 61kB 15.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 71kB 16.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from d2l) (1.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from d2l) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from d2l) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from d2l) (1.19.5)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l) (5.6.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l) (5.0.2)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l) (5.2.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l) (5.3.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l) (4.10.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l) (7.6.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l) (2.4.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->d2l) (2018.9)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (0.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (4.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (1.4.3)\n",
            "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (5.1.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (3.3.0)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (2.11.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (0.6.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (4.3.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l) (0.4.4)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l) (0.2.0)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l) (22.0.2)\n",
            "Requirement already satisfied: jupyter-client>=4.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l) (5.3.5)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l) (1.9.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->d2l) (1.0.18)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->d2l) (5.5.0)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l) (1.5.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l) (0.9.2)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l) (5.1.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->d2l) (1.0.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->d2l) (3.5.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->d2l) (1.15.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert->jupyter->d2l) (2.6.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->d2l) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->d2l) (20.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->jupyter->d2l) (1.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert->jupyter->d2l) (4.4.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->d2l) (0.2.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->d2l) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->d2l) (53.0.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->d2l) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->d2l) (0.7.5)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook->jupyter->d2l) (0.7.0)\n",
            "Installing collected packages: d2l\n",
            "Successfully installed d2l-0.16.1\n",
            "Collecting mxnet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/20/76af36cad6754a15f39d3bff19e09921dec72b85261e455d4edc50ebffa8/mxnet-1.7.0.post2-py2.py3-none-manylinux2014_x86_64.whl (54.7MB)\n",
            "\u001b[K     |████████████████████████████████| 54.7MB 55kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (2.23.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (1.19.5)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
            "Installing collected packages: graphviz, mxnet\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-1.7.0.post2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm_jg5fBUp7J"
      },
      "source": [
        "We downloaded the data into the current directory. To load the two CSV (Comma Separated Values) files containing training and test data respectively we use Pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-12T19:52:59.907368Z",
          "start_time": "2019-02-12T19:52:59.418639Z"
        },
        "attributes": {
          "classes": [],
          "id": "",
          "n": "14"
        },
        "id": "ER93IY5WUp7J"
      },
      "source": [
        "utils.download('http://d2l-data.s3-accelerate.amazonaws.com/kaggle_house_pred_train.csv')\n",
        "utils.download('http://d2l-data.s3-accelerate.amazonaws.com/kaggle_house_pred_test.csv')\n",
        "train_data = pd.read_csv('kaggle_house_pred_train.csv')\n",
        "test_data = pd.read_csv('kaggle_house_pred_test.csv')"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f3wifujUp7K"
      },
      "source": [
        "The training data set includes 1,460 examples, 80 features, and 1 label., the test data contains 1,459 examples and 80 features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-12T19:52:59.938906Z",
          "start_time": "2019-02-12T19:52:59.914530Z"
        },
        "attributes": {
          "classes": [],
          "id": "",
          "n": "11"
        },
        "id": "ILjdGqZJUp7K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c633f641-80ff-4fc7-c245-053451d9f236"
      },
      "source": [
        "print(train_data.shape)\n",
        "print(test_data.shape)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1460, 81)\n",
            "(1459, 80)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAhbVnk1Up7K"
      },
      "source": [
        "Let’s take a look at the first 4 and last 2 features as well as the label (SalePrice) from the first 4 examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-12T19:53:00.162068Z",
          "start_time": "2019-02-12T19:52:59.951154Z"
        },
        "attributes": {
          "classes": [],
          "id": "",
          "n": "28"
        },
        "id": "2i4IQFpjUp7L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "outputId": "8d619027-5245-4513-9399-4a13403949d2"
      },
      "source": [
        "train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]]"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>MSZoning</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>SaleType</th>\n",
              "      <th>SaleCondition</th>\n",
              "      <th>SalePrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>65.0</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>208500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>RL</td>\n",
              "      <td>80.0</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>181500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>68.0</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>223500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>70</td>\n",
              "      <td>RL</td>\n",
              "      <td>60.0</td>\n",
              "      <td>WD</td>\n",
              "      <td>Abnorml</td>\n",
              "      <td>140000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id  MSSubClass MSZoning  LotFrontage SaleType SaleCondition  SalePrice\n",
              "0   1          60       RL         65.0       WD        Normal     208500\n",
              "1   2          20       RL         80.0       WD        Normal     181500\n",
              "2   3          60       RL         68.0       WD        Normal     223500\n",
              "3   4          70       RL         60.0       WD       Abnorml     140000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywpnAzwsUp7L"
      },
      "source": [
        "We can see that in each example, the first feature is the ID. This helps the model identify each training example. While this is convenient, it doesn't carry any information for prediction purposes. Hence we remove it from the dataset before feeding the data into the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-12T19:53:00.379027Z",
          "start_time": "2019-02-12T19:53:00.166246Z"
        },
        "attributes": {
          "classes": [],
          "id": "",
          "n": "30"
        },
        "id": "3SZJKsi0Up7L"
      },
      "source": [
        "all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBp_6EnxUp7L"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "As stated above, we have a wide variety of datatypes. Before we feed it into a deep network we need to perform some amount of processing. Let's start with the numerical features. We begin by replacing missing values with the mean. This is a reasonable strategy if features are missing at random. To adjust them to a common scale we rescale them to zero mean and unit variance. This is accomplished as follows:\n",
        "\n",
        "$$x \\leftarrow \\frac{x - \\mu}{\\sigma}$$\n",
        "\n",
        "To check that this transforms $x$ to data with zero mean and unit variance simply calculate $\\mathbf{E}[(x-\\mu)/\\sigma] = (\\mu - \\mu)/\\sigma = 0$. To check the variance we use $\\mathbf{E}[(x-\\mu)^2] = \\sigma^2$ and thus the transformed variable has unit variance. The reason for 'normalizing' the data is that it brings all features to the same order of magnitude. After all, we do not know *a priori* which features are likely to be relevant. Hence it makes sense to treat them equally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M14NHCaJtHuT"
      },
      "source": [
        "numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QL5dgItc4qf"
      },
      "source": [
        "#adding indicator variables\r\n",
        "nan_values = all_features.isna()\r\n",
        "nan_columns = nan_values.any()\r\n",
        "\r\n",
        "columns_with_nan = all_features.columns[nan_columns].tolist()\r\n",
        "\r\n",
        "df = all_features[columns_with_nan].isnull().astype(int).add_suffix('_indicator')\r\n",
        "def mygen(lst):\r\n",
        "    for item in lst:\r\n",
        "        yield item\r\n",
        "        yield item + '_indicator'\r\n",
        "\r\n",
        "all_features = pd.concat([all_features, df],axis=1)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-12T19:53:01.109173Z",
          "start_time": "2019-02-12T19:53:00.389736Z"
        },
        "attributes": {
          "classes": [],
          "id": "",
          "n": "6"
        },
        "id": "VuXl3UOTUp7M"
      },
      "source": [
        "numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index\n",
        "all_features[numeric_features] = all_features[numeric_features].apply(\n",
        "    lambda x: (x - x.mean()) / (x.std()))\n",
        "# after standardizing the data all means vanish, hence we can set missing values to 0\n",
        "all_features = all_features.fillna(0)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKy2viGIUp7M"
      },
      "source": [
        "Next we deal with discrete values. This includes variables such as 'MSZoning'. We replace them by a one-hot encoding in the same manner as how we transformed multiclass classification data into a vector of $0$ and $1$. For instance, 'MSZoning' assumes the values 'RL' and 'RM'. They map into vectors $(1,0)$ and $(0,1)$ respectively. Pandas does this automatically for us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-12T19:53:02.149003Z",
          "start_time": "2019-02-12T19:53:01.124831Z"
        },
        "attributes": {
          "classes": [],
          "id": "",
          "n": "7"
        },
        "id": "pPYH64xcUp7M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ef8a5ec-5b19-467a-898c-a06cb1d04343"
      },
      "source": [
        "# Dummy_na=True refers to a missing value being a legal eigenvalue, and creates an indicative feature for it.\n",
        "all_features = pd.get_dummies(all_features, dummy_na=True)\n",
        "all_features.shape"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2919, 388)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z47c7c7cG_wk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "ddf46f9b-066f-431b-a718-9a41e19b1475"
      },
      "source": [
        "all_features"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>OverallQual</th>\n",
              "      <th>OverallCond</th>\n",
              "      <th>YearBuilt</th>\n",
              "      <th>YearRemodAdd</th>\n",
              "      <th>MasVnrArea</th>\n",
              "      <th>BsmtFinSF1</th>\n",
              "      <th>BsmtFinSF2</th>\n",
              "      <th>BsmtUnfSF</th>\n",
              "      <th>TotalBsmtSF</th>\n",
              "      <th>1stFlrSF</th>\n",
              "      <th>2ndFlrSF</th>\n",
              "      <th>LowQualFinSF</th>\n",
              "      <th>GrLivArea</th>\n",
              "      <th>BsmtFullBath</th>\n",
              "      <th>BsmtHalfBath</th>\n",
              "      <th>FullBath</th>\n",
              "      <th>HalfBath</th>\n",
              "      <th>BedroomAbvGr</th>\n",
              "      <th>KitchenAbvGr</th>\n",
              "      <th>TotRmsAbvGrd</th>\n",
              "      <th>Fireplaces</th>\n",
              "      <th>GarageYrBlt</th>\n",
              "      <th>GarageCars</th>\n",
              "      <th>GarageArea</th>\n",
              "      <th>WoodDeckSF</th>\n",
              "      <th>OpenPorchSF</th>\n",
              "      <th>EnclosedPorch</th>\n",
              "      <th>3SsnPorch</th>\n",
              "      <th>ScreenPorch</th>\n",
              "      <th>PoolArea</th>\n",
              "      <th>MiscVal</th>\n",
              "      <th>MoSold</th>\n",
              "      <th>YrSold</th>\n",
              "      <th>MSZoning_indicator</th>\n",
              "      <th>LotFrontage_indicator</th>\n",
              "      <th>Alley_indicator</th>\n",
              "      <th>Utilities_indicator</th>\n",
              "      <th>...</th>\n",
              "      <th>GarageCond_nan</th>\n",
              "      <th>PavedDrive_N</th>\n",
              "      <th>PavedDrive_P</th>\n",
              "      <th>PavedDrive_Y</th>\n",
              "      <th>PavedDrive_nan</th>\n",
              "      <th>PoolQC_0</th>\n",
              "      <th>PoolQC_Ex</th>\n",
              "      <th>PoolQC_Fa</th>\n",
              "      <th>PoolQC_Gd</th>\n",
              "      <th>PoolQC_nan</th>\n",
              "      <th>Fence_0</th>\n",
              "      <th>Fence_GdPrv</th>\n",
              "      <th>Fence_GdWo</th>\n",
              "      <th>Fence_MnPrv</th>\n",
              "      <th>Fence_MnWw</th>\n",
              "      <th>Fence_nan</th>\n",
              "      <th>MiscFeature_0</th>\n",
              "      <th>MiscFeature_Gar2</th>\n",
              "      <th>MiscFeature_Othr</th>\n",
              "      <th>MiscFeature_Shed</th>\n",
              "      <th>MiscFeature_TenC</th>\n",
              "      <th>MiscFeature_nan</th>\n",
              "      <th>SaleType_0</th>\n",
              "      <th>SaleType_COD</th>\n",
              "      <th>SaleType_CWD</th>\n",
              "      <th>SaleType_Con</th>\n",
              "      <th>SaleType_ConLD</th>\n",
              "      <th>SaleType_ConLI</th>\n",
              "      <th>SaleType_ConLw</th>\n",
              "      <th>SaleType_New</th>\n",
              "      <th>SaleType_Oth</th>\n",
              "      <th>SaleType_WD</th>\n",
              "      <th>SaleType_nan</th>\n",
              "      <th>SaleCondition_Abnorml</th>\n",
              "      <th>SaleCondition_AdjLand</th>\n",
              "      <th>SaleCondition_Alloca</th>\n",
              "      <th>SaleCondition_Family</th>\n",
              "      <th>SaleCondition_Normal</th>\n",
              "      <th>SaleCondition_Partial</th>\n",
              "      <th>SaleCondition_nan</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.067320</td>\n",
              "      <td>-0.184443</td>\n",
              "      <td>-0.217841</td>\n",
              "      <td>0.646073</td>\n",
              "      <td>-0.507197</td>\n",
              "      <td>1.046078</td>\n",
              "      <td>0.896679</td>\n",
              "      <td>0.523038</td>\n",
              "      <td>0.580708</td>\n",
              "      <td>-0.29303</td>\n",
              "      <td>-0.934542</td>\n",
              "      <td>-0.444176</td>\n",
              "      <td>-0.773728</td>\n",
              "      <td>1.207172</td>\n",
              "      <td>-0.10118</td>\n",
              "      <td>0.413476</td>\n",
              "      <td>1.086464</td>\n",
              "      <td>-0.249767</td>\n",
              "      <td>0.781232</td>\n",
              "      <td>1.232388</td>\n",
              "      <td>0.169898</td>\n",
              "      <td>-0.207663</td>\n",
              "      <td>0.986680</td>\n",
              "      <td>-0.924153</td>\n",
              "      <td>0.973110</td>\n",
              "      <td>0.306423</td>\n",
              "      <td>0.348780</td>\n",
              "      <td>-0.740634</td>\n",
              "      <td>0.199972</td>\n",
              "      <td>-0.359539</td>\n",
              "      <td>-0.103313</td>\n",
              "      <td>-0.285886</td>\n",
              "      <td>-0.063139</td>\n",
              "      <td>-0.089577</td>\n",
              "      <td>-1.551918</td>\n",
              "      <td>0.157619</td>\n",
              "      <td>-0.037037</td>\n",
              "      <td>-0.446861</td>\n",
              "      <td>0.269708</td>\n",
              "      <td>-0.02618</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.873466</td>\n",
              "      <td>0.458096</td>\n",
              "      <td>-0.072032</td>\n",
              "      <td>-0.063174</td>\n",
              "      <td>2.187904</td>\n",
              "      <td>0.154737</td>\n",
              "      <td>-0.395536</td>\n",
              "      <td>-0.569893</td>\n",
              "      <td>1.177709</td>\n",
              "      <td>-0.29303</td>\n",
              "      <td>-0.629681</td>\n",
              "      <td>0.476948</td>\n",
              "      <td>0.261030</td>\n",
              "      <td>-0.784891</td>\n",
              "      <td>-0.10118</td>\n",
              "      <td>-0.471810</td>\n",
              "      <td>-0.819258</td>\n",
              "      <td>3.820454</td>\n",
              "      <td>0.781232</td>\n",
              "      <td>-0.756191</td>\n",
              "      <td>0.169898</td>\n",
              "      <td>-0.207663</td>\n",
              "      <td>-0.287709</td>\n",
              "      <td>0.623525</td>\n",
              "      <td>-0.082638</td>\n",
              "      <td>0.306423</td>\n",
              "      <td>-0.059772</td>\n",
              "      <td>1.614603</td>\n",
              "      <td>-0.702722</td>\n",
              "      <td>-0.359539</td>\n",
              "      <td>-0.103313</td>\n",
              "      <td>-0.285886</td>\n",
              "      <td>-0.063139</td>\n",
              "      <td>-0.089577</td>\n",
              "      <td>-0.446848</td>\n",
              "      <td>-0.602858</td>\n",
              "      <td>-0.037037</td>\n",
              "      <td>-0.446861</td>\n",
              "      <td>0.269708</td>\n",
              "      <td>-0.02618</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.067320</td>\n",
              "      <td>-0.055935</td>\n",
              "      <td>0.137173</td>\n",
              "      <td>0.646073</td>\n",
              "      <td>-0.507197</td>\n",
              "      <td>0.980053</td>\n",
              "      <td>0.848819</td>\n",
              "      <td>0.333448</td>\n",
              "      <td>0.097840</td>\n",
              "      <td>-0.29303</td>\n",
              "      <td>-0.288418</td>\n",
              "      <td>-0.298974</td>\n",
              "      <td>-0.610614</td>\n",
              "      <td>1.235163</td>\n",
              "      <td>-0.10118</td>\n",
              "      <td>0.563659</td>\n",
              "      <td>1.086464</td>\n",
              "      <td>-0.249767</td>\n",
              "      <td>0.781232</td>\n",
              "      <td>1.232388</td>\n",
              "      <td>0.169898</td>\n",
              "      <td>-0.207663</td>\n",
              "      <td>-0.287709</td>\n",
              "      <td>0.623525</td>\n",
              "      <td>0.894907</td>\n",
              "      <td>0.306423</td>\n",
              "      <td>0.627338</td>\n",
              "      <td>-0.740634</td>\n",
              "      <td>-0.081195</td>\n",
              "      <td>-0.359539</td>\n",
              "      <td>-0.103313</td>\n",
              "      <td>-0.285886</td>\n",
              "      <td>-0.063139</td>\n",
              "      <td>-0.089577</td>\n",
              "      <td>1.026577</td>\n",
              "      <td>0.157619</td>\n",
              "      <td>-0.037037</td>\n",
              "      <td>-0.446861</td>\n",
              "      <td>0.269708</td>\n",
              "      <td>-0.02618</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.302516</td>\n",
              "      <td>-0.398622</td>\n",
              "      <td>-0.078371</td>\n",
              "      <td>0.646073</td>\n",
              "      <td>-0.507197</td>\n",
              "      <td>-1.859033</td>\n",
              "      <td>-0.682695</td>\n",
              "      <td>-0.569893</td>\n",
              "      <td>-0.494771</td>\n",
              "      <td>-0.29303</td>\n",
              "      <td>-0.047258</td>\n",
              "      <td>-0.671053</td>\n",
              "      <td>-0.506118</td>\n",
              "      <td>0.978574</td>\n",
              "      <td>-0.10118</td>\n",
              "      <td>0.427309</td>\n",
              "      <td>1.086464</td>\n",
              "      <td>-0.249767</td>\n",
              "      <td>-1.027187</td>\n",
              "      <td>-0.756191</td>\n",
              "      <td>0.169898</td>\n",
              "      <td>-0.207663</td>\n",
              "      <td>0.349486</td>\n",
              "      <td>0.623525</td>\n",
              "      <td>0.777601</td>\n",
              "      <td>1.619406</td>\n",
              "      <td>0.785188</td>\n",
              "      <td>-0.740634</td>\n",
              "      <td>-0.184783</td>\n",
              "      <td>3.874303</td>\n",
              "      <td>-0.103313</td>\n",
              "      <td>-0.285886</td>\n",
              "      <td>-0.063139</td>\n",
              "      <td>-0.089577</td>\n",
              "      <td>-1.551918</td>\n",
              "      <td>-1.363335</td>\n",
              "      <td>-0.037037</td>\n",
              "      <td>-0.446861</td>\n",
              "      <td>0.269708</td>\n",
              "      <td>-0.02618</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.067320</td>\n",
              "      <td>0.629439</td>\n",
              "      <td>0.518814</td>\n",
              "      <td>1.355319</td>\n",
              "      <td>-0.507197</td>\n",
              "      <td>0.947040</td>\n",
              "      <td>0.753100</td>\n",
              "      <td>1.381770</td>\n",
              "      <td>0.468770</td>\n",
              "      <td>-0.29303</td>\n",
              "      <td>-0.161013</td>\n",
              "      <td>0.211501</td>\n",
              "      <td>-0.037164</td>\n",
              "      <td>1.671364</td>\n",
              "      <td>-0.10118</td>\n",
              "      <td>1.377806</td>\n",
              "      <td>1.086464</td>\n",
              "      <td>-0.249767</td>\n",
              "      <td>0.781232</td>\n",
              "      <td>1.232388</td>\n",
              "      <td>1.385418</td>\n",
              "      <td>-0.207663</td>\n",
              "      <td>1.623875</td>\n",
              "      <td>0.623525</td>\n",
              "      <td>0.855805</td>\n",
              "      <td>1.619406</td>\n",
              "      <td>1.685860</td>\n",
              "      <td>0.776834</td>\n",
              "      <td>0.540332</td>\n",
              "      <td>-0.359539</td>\n",
              "      <td>-0.103313</td>\n",
              "      <td>-0.285886</td>\n",
              "      <td>-0.063139</td>\n",
              "      <td>-0.089577</td>\n",
              "      <td>2.131647</td>\n",
              "      <td>0.157619</td>\n",
              "      <td>-0.037037</td>\n",
              "      <td>-0.446861</td>\n",
              "      <td>0.269708</td>\n",
              "      <td>-0.02618</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1454</th>\n",
              "      <td>2.419286</td>\n",
              "      <td>-2.069222</td>\n",
              "      <td>-1.043758</td>\n",
              "      <td>-1.481667</td>\n",
              "      <td>1.289537</td>\n",
              "      <td>-0.043338</td>\n",
              "      <td>-0.682695</td>\n",
              "      <td>-0.569893</td>\n",
              "      <td>-0.968860</td>\n",
              "      <td>-0.29303</td>\n",
              "      <td>-0.033608</td>\n",
              "      <td>-1.147496</td>\n",
              "      <td>-1.563815</td>\n",
              "      <td>0.488723</td>\n",
              "      <td>-0.10118</td>\n",
              "      <td>-0.807744</td>\n",
              "      <td>-0.819258</td>\n",
              "      <td>-0.249767</td>\n",
              "      <td>-1.027187</td>\n",
              "      <td>1.232388</td>\n",
              "      <td>0.169898</td>\n",
              "      <td>-0.207663</td>\n",
              "      <td>-0.924904</td>\n",
              "      <td>-0.924153</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.319544</td>\n",
              "      <td>-2.195385</td>\n",
              "      <td>-0.740634</td>\n",
              "      <td>-0.702722</td>\n",
              "      <td>-0.359539</td>\n",
              "      <td>-0.103313</td>\n",
              "      <td>-0.285886</td>\n",
              "      <td>-0.063139</td>\n",
              "      <td>-0.089577</td>\n",
              "      <td>-0.078492</td>\n",
              "      <td>-1.363335</td>\n",
              "      <td>-0.037037</td>\n",
              "      <td>-0.446861</td>\n",
              "      <td>0.269708</td>\n",
              "      <td>-0.02618</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1455</th>\n",
              "      <td>2.419286</td>\n",
              "      <td>-2.069222</td>\n",
              "      <td>-1.049083</td>\n",
              "      <td>-1.481667</td>\n",
              "      <td>-0.507197</td>\n",
              "      <td>-0.043338</td>\n",
              "      <td>-0.682695</td>\n",
              "      <td>-0.569893</td>\n",
              "      <td>-0.415757</td>\n",
              "      <td>-0.29303</td>\n",
              "      <td>-0.606930</td>\n",
              "      <td>-1.147496</td>\n",
              "      <td>-1.563815</td>\n",
              "      <td>0.488723</td>\n",
              "      <td>-0.10118</td>\n",
              "      <td>-0.807744</td>\n",
              "      <td>-0.819258</td>\n",
              "      <td>-0.249767</td>\n",
              "      <td>-1.027187</td>\n",
              "      <td>1.232388</td>\n",
              "      <td>0.169898</td>\n",
              "      <td>-0.207663</td>\n",
              "      <td>-0.287709</td>\n",
              "      <td>-0.924153</td>\n",
              "      <td>-0.317249</td>\n",
              "      <td>-1.006561</td>\n",
              "      <td>-0.867591</td>\n",
              "      <td>-0.740634</td>\n",
              "      <td>-0.347564</td>\n",
              "      <td>-0.359539</td>\n",
              "      <td>-0.103313</td>\n",
              "      <td>-0.285886</td>\n",
              "      <td>-0.063139</td>\n",
              "      <td>-0.089577</td>\n",
              "      <td>-0.815205</td>\n",
              "      <td>-1.363335</td>\n",
              "      <td>-0.037037</td>\n",
              "      <td>-0.446861</td>\n",
              "      <td>0.269708</td>\n",
              "      <td>-0.02618</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1456</th>\n",
              "      <td>-0.873466</td>\n",
              "      <td>3.884968</td>\n",
              "      <td>1.246594</td>\n",
              "      <td>-0.772420</td>\n",
              "      <td>1.289537</td>\n",
              "      <td>-0.373465</td>\n",
              "      <td>0.561660</td>\n",
              "      <td>-0.569893</td>\n",
              "      <td>1.717643</td>\n",
              "      <td>-0.29303</td>\n",
              "      <td>-1.275805</td>\n",
              "      <td>0.390734</td>\n",
              "      <td>0.164181</td>\n",
              "      <td>-0.784891</td>\n",
              "      <td>-0.10118</td>\n",
              "      <td>-0.546901</td>\n",
              "      <td>1.086464</td>\n",
              "      <td>-0.249767</td>\n",
              "      <td>-1.027187</td>\n",
              "      <td>-0.756191</td>\n",
              "      <td>1.385418</td>\n",
              "      <td>-0.207663</td>\n",
              "      <td>0.349486</td>\n",
              "      <td>0.623525</td>\n",
              "      <td>-0.708266</td>\n",
              "      <td>0.306423</td>\n",
              "      <td>0.478774</td>\n",
              "      <td>3.005615</td>\n",
              "      <td>-0.702722</td>\n",
              "      <td>-0.359539</td>\n",
              "      <td>-0.103313</td>\n",
              "      <td>-0.285886</td>\n",
              "      <td>-0.063139</td>\n",
              "      <td>-0.089577</td>\n",
              "      <td>1.026577</td>\n",
              "      <td>-1.363335</td>\n",
              "      <td>-0.037037</td>\n",
              "      <td>-0.446861</td>\n",
              "      <td>0.269708</td>\n",
              "      <td>-0.02618</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1457</th>\n",
              "      <td>0.655311</td>\n",
              "      <td>-0.312950</td>\n",
              "      <td>0.034599</td>\n",
              "      <td>-0.772420</td>\n",
              "      <td>-0.507197</td>\n",
              "      <td>0.682939</td>\n",
              "      <td>0.370221</td>\n",
              "      <td>-0.569893</td>\n",
              "      <td>-0.229194</td>\n",
              "      <td>-0.29303</td>\n",
              "      <td>0.032370</td>\n",
              "      <td>-0.317124</td>\n",
              "      <td>-0.483181</td>\n",
              "      <td>-0.784891</td>\n",
              "      <td>-0.10118</td>\n",
              "      <td>-1.048827</td>\n",
              "      <td>-0.819258</td>\n",
              "      <td>3.820454</td>\n",
              "      <td>-1.027187</td>\n",
              "      <td>-0.756191</td>\n",
              "      <td>0.169898</td>\n",
              "      <td>-0.207663</td>\n",
              "      <td>-0.287709</td>\n",
              "      <td>-0.924153</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.319544</td>\n",
              "      <td>-2.195385</td>\n",
              "      <td>-0.108355</td>\n",
              "      <td>-0.229178</td>\n",
              "      <td>-0.359539</td>\n",
              "      <td>-0.103313</td>\n",
              "      <td>-0.285886</td>\n",
              "      <td>-0.063139</td>\n",
              "      <td>1.144116</td>\n",
              "      <td>0.289865</td>\n",
              "      <td>-1.363335</td>\n",
              "      <td>-0.037037</td>\n",
              "      <td>-0.446861</td>\n",
              "      <td>0.269708</td>\n",
              "      <td>-0.02618</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1458</th>\n",
              "      <td>0.067320</td>\n",
              "      <td>0.201080</td>\n",
              "      <td>-0.068608</td>\n",
              "      <td>0.646073</td>\n",
              "      <td>-0.507197</td>\n",
              "      <td>0.715952</td>\n",
              "      <td>0.465941</td>\n",
              "      <td>-0.045732</td>\n",
              "      <td>0.694840</td>\n",
              "      <td>-0.29303</td>\n",
              "      <td>-0.734335</td>\n",
              "      <td>-0.126547</td>\n",
              "      <td>-0.416915</td>\n",
              "      <td>1.557066</td>\n",
              "      <td>-0.10118</td>\n",
              "      <td>0.986541</td>\n",
              "      <td>-0.819258</td>\n",
              "      <td>-0.249767</td>\n",
              "      <td>0.781232</td>\n",
              "      <td>1.232388</td>\n",
              "      <td>0.169898</td>\n",
              "      <td>-0.207663</td>\n",
              "      <td>1.623875</td>\n",
              "      <td>0.623525</td>\n",
              "      <td>0.582092</td>\n",
              "      <td>1.619406</td>\n",
              "      <td>0.822329</td>\n",
              "      <td>0.761027</td>\n",
              "      <td>0.007594</td>\n",
              "      <td>-0.359539</td>\n",
              "      <td>-0.103313</td>\n",
              "      <td>-0.285886</td>\n",
              "      <td>-0.063139</td>\n",
              "      <td>-0.089577</td>\n",
              "      <td>1.763290</td>\n",
              "      <td>-1.363335</td>\n",
              "      <td>-0.037037</td>\n",
              "      <td>-0.446861</td>\n",
              "      <td>0.269708</td>\n",
              "      <td>-0.02618</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2919 rows × 388 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      MSSubClass  LotFrontage  ...  SaleCondition_Partial  SaleCondition_nan\n",
              "0       0.067320    -0.184443  ...                      0                  0\n",
              "1      -0.873466     0.458096  ...                      0                  0\n",
              "2       0.067320    -0.055935  ...                      0                  0\n",
              "3       0.302516    -0.398622  ...                      0                  0\n",
              "4       0.067320     0.629439  ...                      0                  0\n",
              "...          ...          ...  ...                    ...                ...\n",
              "1454    2.419286    -2.069222  ...                      0                  0\n",
              "1455    2.419286    -2.069222  ...                      0                  0\n",
              "1456   -0.873466     3.884968  ...                      0                  0\n",
              "1457    0.655311    -0.312950  ...                      0                  0\n",
              "1458    0.067320     0.201080  ...                      0                  0\n",
              "\n",
              "[2919 rows x 388 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyRb0u6dUp7M"
      },
      "source": [
        "You can see that this conversion increases the number of features from 79 to 331. Finally, via the `values` attribute we can extract the NumPy format from the Pandas dataframe and convert it into MXNet's native representation - NDArray for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-12T19:53:02.295819Z",
          "start_time": "2019-02-12T19:53:02.153731Z"
        },
        "attributes": {
          "classes": [],
          "id": "",
          "n": "9"
        },
        "id": "MygSL9rRUp7N"
      },
      "source": [
        "n_train = train_data.shape[0]\n",
        "train_features = nd.array(all_features[:n_train].values)\n",
        "test_features = nd.array(all_features[n_train:].values)\n",
        "train_labels = nd.array(train_data.SalePrice.values).reshape((-1, 1))\n",
        "#train_labels = nd.log(nd.array(train_data.SalePrice.values).reshape((-1, 1)))"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPcp6dBeUp7N"
      },
      "source": [
        "## Training\n",
        "\n",
        "To get started we train a linear model with squared loss. This will obviously not lead to a competition winning submission but it provides a sanity check to see whether there's meaningful information in the data. It also amounts to a minimum baseline of how well we should expect any 'fancy' model to work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-12T19:53:02.347358Z",
          "start_time": "2019-02-12T19:53:02.310339Z"
        },
        "attributes": {
          "classes": [],
          "id": "",
          "n": "13"
        },
        "id": "N1nUVpDqUp7N"
      },
      "source": [
        "loss = gloss.L2Loss()\n",
        "\n",
        "def get_net():\n",
        "    net = nn.Sequential()\n",
        "   # net.add(nn.Dense(units=354, activation='relu'))\n",
        "    #net.add(nn.Dense(units=300, in_units=354, activation='relu'))      \n",
        "    net.add(nn.Dense(units=1))\n",
        "    net.initialize()\n",
        "    return net"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgAu5Be0Up7N"
      },
      "source": [
        "House prices, like shares, are relative. That is, we probably care more about the relative error $\\frac{y - \\hat{y}}{y}$ than about the absolute error. For instance, getting a house price wrong by USD 100,000 is terrible in Rural Ohio, where the value of the house is USD 125,000. On the other hand, if we err by this amount in Los Altos Hills, California, we can be proud of the accuracy of our model (the median house price there exceeds 4 million).\n",
        "\n",
        "One way to address this problem is to measure the discrepancy in the logarithm of the price estimates. In fact, this is also the error that is being used to measure the quality in this competition. After all, a small value $\\delta$ of $\\log y - \\log \\hat{y}$ translates into $e^{-\\delta} \\leq \\frac{\\hat{y}}{y} \\leq e^\\delta$. This leads to the following loss function:\n",
        "\n",
        "$$L = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n\\left(\\log y_i -\\log \\hat{y}_i\\right)^2}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-12T19:53:02.388615Z",
          "start_time": "2019-02-12T19:53:02.356508Z"
        },
        "attributes": {
          "classes": [],
          "id": "",
          "n": "11"
        },
        "id": "xkLl9r6TUp7N"
      },
      "source": [
        "def log_rmse(net, features, labels):\n",
        "    # To further stabilize the value when the logarithm is taken, set the value less than 1 as 1.\n",
        "    clipped_preds = nd.clip(net(features), 1, float('inf'))\n",
        "    rmse = nd.sqrt(2 * loss(clipped_preds.log(), labels.log()).mean())\n",
        "    return rmse.asscalar()"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUgo7Q5fUp7N"
      },
      "source": [
        "Unlike in the previous sections, the following training functions use the Adam optimization algorithm.  Compared to the previously used mini-batch stochastic gradient descent, the Adam optimization algorithm is relatively less sensitive to learning rates.  This will be covered in further detail later on when we discuss the details on [Optimization Algorithms](../chapter_optimization/index.md) in a separate chapter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-12T19:53:02.478944Z",
          "start_time": "2019-02-12T19:53:02.410348Z"
        },
        "attributes": {
          "classes": [],
          "id": "",
          "n": "14"
        },
        "id": "3fPTUmN8Up7N"
      },
      "source": [
        "def train(net, train_features, train_labels, test_features, test_labels,\n",
        "          num_epochs, learning_rate, weight_decay, batch_size):\n",
        "    train_ls, test_ls = [], []\n",
        "    train_iter = gdata.DataLoader(gdata.ArrayDataset(\n",
        "        train_features, train_labels), batch_size, shuffle=True)\n",
        "    # The Adam optimization algorithm is used here.\n",
        "    trainer = gluon.Trainer(net.collect_params(), 'adam', {\n",
        "        'learning_rate': learning_rate, 'wd': weight_decay})\n",
        "    for epoch in range(num_epochs):\n",
        "        for X, y in train_iter:\n",
        "            with autograd.record():\n",
        "                l = loss(net(X), y)\n",
        "            l.backward()\n",
        "            trainer.step(batch_size)\n",
        "        train_ls.append(log_rmse(net, train_features, train_labels))\n",
        "        if test_labels is not None:\n",
        "            test_ls.append(log_rmse(net, test_features, test_labels))\n",
        "    return train_ls, test_ls"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avX897B2Up7O"
      },
      "source": [
        "## k-Fold Cross-Validation\n",
        "\n",
        "The k-fold cross-validation was introduced in the section where we discussed how to deal with [“Model Selection, Underfitting and Overfitting\"](underfit-overfit.md). We will put this to good use to select the model design and to adjust the hyperparameters. We first need a function that returns the i-th fold of the data in a k-fold cros-validation procedure. It proceeds by slicing out the i-th segment as validation data and returning the rest as training data. Note - this is not the most efficient way of handling data and we would use something much smarter if the amount of data was considerably larger. But this would obscure the function of the code considerably and we thus omit it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-12T19:53:02.558010Z",
          "start_time": "2019-02-12T19:53:02.507447Z"
        },
        "id": "UL2D0LMPUp7O"
      },
      "source": [
        "def get_k_fold_data(k, i, X, y):\n",
        "    assert k > 1\n",
        "    fold_size = X.shape[0] // k\n",
        "    X_train, y_train = None, None\n",
        "    for j in range(k):\n",
        "        idx = slice(j * fold_size, (j + 1) * fold_size)\n",
        "        X_part, y_part = X[idx, :], y[idx]\n",
        "        if j == i:\n",
        "            X_valid, y_valid = X_part, y_part\n",
        "        elif X_train is None:\n",
        "            X_train, y_train = X_part, y_part\n",
        "        else:\n",
        "            X_train = nd.concat(X_train, X_part, dim=0)\n",
        "            y_train = nd.concat(y_train, y_part, dim=0)\n",
        "    return X_train, y_train, X_valid, y_valid"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-Rg55iEUp7O"
      },
      "source": [
        "The training and verification error averages are returned when we train $k$ times in the k-fold cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-12T19:53:02.601248Z",
          "start_time": "2019-02-12T19:53:02.576663Z"
        },
        "attributes": {
          "classes": [],
          "id": "",
          "n": "15"
        },
        "id": "KkhsZbBQUp7O"
      },
      "source": [
        "def k_fold(k, X_train, y_train, num_epochs,\n",
        "           learning_rate, weight_decay, batch_size):\n",
        "    train_l_sum, valid_l_sum = 0, 0\n",
        "    for i in range(k):\n",
        "        data = get_k_fold_data(k, i, X_train, y_train)\n",
        "        net = get_net()\n",
        "        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,\n",
        "                                   weight_decay, batch_size)\n",
        "        train_l_sum += train_ls[-1]\n",
        "        valid_l_sum += valid_ls[-1]\n",
        "        if i == 0:\n",
        "            plt.semilogy(range(1, num_epochs + 1), train_ls)\n",
        "            plt.semilogy(range(1, num_epochs + 1), valid_ls)\n",
        "            plt.legend(['train', 'valid'])\n",
        "        print('fold %d, train rmse: %f, valid rmse: %f' % (\n",
        "            i, train_ls[-1], valid_ls[-1]))\n",
        "    return train_l_sum / k, valid_l_sum / k"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P27YppP7Up7O"
      },
      "source": [
        "## Model Selection\n",
        "\n",
        "We pick a rather un-tuned set of hyperparameters and leave it up to the reader to improve the model considerably. Finding a good choice can take quite some time, depending on how many things one wants to optimize over. Within reason the k-fold crossvalidation approach is resilient against multiple testing. However, if we were to try out an unreasonably large number of options it might fail since we might just get lucky on the validation split with a particular set of hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-12T19:53:02.603202Z",
          "start_time": "2019-02-12T19:52:28.070Z"
        },
        "attributes": {
          "classes": [],
          "id": "",
          "n": "16"
        },
        "id": "oSy8Tge4Up7O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "8ae8b630-53b1-498a-8487-1d6725e60363"
      },
      "source": [
        "#k, num_epochs, lr, weight_decay, batch_size = 5, 150, .6, 0.01, 50 <<-- best hyperparameters for 354, 354, 1 network\n",
        "#k, num_epochs, lr, weight_decay, batch_size = 5, 150, 3, 0.01, 20 <<-- best hyperparameters for no indicator variables base network\n",
        "k, num_epochs, lr, weight_decay, batch_size = 5, 150, 6, 0.02, 20\n",
        "train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr,\n",
        "                          weight_decay, batch_size)\n",
        "print('%d-fold validation: avg train rmse: %f, avg valid rmse: %f'\n",
        "      % (k, train_l, valid_l))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fold 0, train rmse: 0.132318, valid rmse: 0.141294\n",
            "fold 1, train rmse: 0.128076, valid rmse: 0.151063\n",
            "fold 2, train rmse: 0.127490, valid rmse: 0.149285\n",
            "fold 3, train rmse: 0.132689, valid rmse: 0.135558\n",
            "fold 4, train rmse: 0.124570, valid rmse: 0.161787\n",
            "5-fold validation: avg train rmse: 0.129028, avg valid rmse: 0.147797\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0c00itX8Hq2"
      },
      "source": [
        "**Notes and Personal Observations about tuning hyper parameters and creating a good network:**\r\n",
        "\r\n",
        "*   The optimal learning rate can varry wildly based on the structure of the network - I find that with a more complictaed network a smaller learning rate is typically better, but for simpler networks larger learning rates are better\r\n",
        "*   I find that any amount of epochs under 150 does't allow the model to train for long enough and converge\r\n",
        "*   Calculating the log of the prices, vs just the prices causes the validation error to more closely follow the training error - I belive this is because on a log scale the errors the predicted and label values would naturalluy be closer together - HOWEVER, I am choosing not to predict the log of the prices because when I go to convert the test predicitons back to normal scale (np.exp) Some of my values go to infinity\r\n",
        "*   I have tried several configurations of vanilla feed-forward NNS - I found that the best performing ones were with 2 hidden layers of 354 nodes or just one dense layer (orignal) - I belive that the complexity of the neural network just causes a lot of overfitting and doesn't improve accuracy on the test set. This is why I prioritized tuning the hyper parameters for the simpler models.\r\n",
        "*   I am using an indicator variable to indicate if there are missing values. An example of when it would be innapropriate to use the mean to deal with missing values is when dealing with a feature that represents a charecteristic of a house. For example, some houses just don't have allys by them depending on where they are located - it would be smarter to use an indicator to indicate if there is an ally or not.\r\n",
        "*   List item\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U37SL6x9Up7O"
      },
      "source": [
        "You will notice that sometimes the number of training errors for a set of hyper-parameters can be very low, while the number of errors for the $K$-fold cross validation may be higher. This is most likely a consequence of overfitting. Therefore, when we reduce the amount of training errors, we need to check whether the amount of errors in the k-fold cross-validation have also been reduced accordingly.\n",
        "\n",
        "##  Predict and Submit\n",
        "\n",
        "Now that we know what a good choice of hyperparameters should be, we might as well use all the data to train on it (rather than just $1-1/k$ of the data that is used in the crossvalidation slices). The model that we obtain in this way can then be applied to the test set. Saving the estimates in a CSV file will simplify uploading the results to Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-12T19:53:02.605253Z",
          "start_time": "2019-02-12T19:52:28.073Z"
        },
        "attributes": {
          "classes": [],
          "id": "",
          "n": "18"
        },
        "id": "59YPNKHjUp7O"
      },
      "source": [
        "def train_and_pred(train_features, test_feature, train_labels, test_data,\n",
        "                   num_epochs, lr, weight_decay, batch_size):\n",
        "    net = get_net()\n",
        "    train_ls, _ = train(net, train_features, train_labels, None, None,\n",
        "                        num_epochs, lr, weight_decay, batch_size)\n",
        "    plt.semilogy(range(1, num_epochs + 1), train_ls)\n",
        "    print('train rmse %f' % train_ls[-1])\n",
        "    # apply the network to the test set\n",
        "    preds = (net(test_features).asnumpy())\n",
        "    # reformat it for export to Kaggle\n",
        "    test_data['SalePrice'] = pd.Series(preds.reshape(1, -1)[0])\n",
        "    print(test_data['SalePrice'])\n",
        "    submission = pd.concat([test_data['Id'], test_data['SalePrice']], axis=1)\n",
        "    submission.to_csv('submission.csv', index=False)\n",
        "    files.download(\"submission.csv\")"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VvN7g4XUp7O"
      },
      "source": [
        "Let's invoke the model. A good sanity check is to see whether the predictions on the test set resemble those of the k-fold crossvalication process. If they do, it's time to upload them to Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-12T19:53:02.607757Z",
          "start_time": "2019-02-12T19:52:28.075Z"
        },
        "attributes": {
          "classes": [],
          "id": "",
          "n": "19"
        },
        "id": "7vsy0DX8Up7O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "outputId": "e4687fc6-9708-4b22-d08f-2e462bc2ccde"
      },
      "source": [
        "train_and_pred(train_features, test_features, train_labels, test_data,\n",
        "               num_epochs, lr, weight_decay, batch_size)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train rmse 0.129396\n",
            "0       111743.171875\n",
            "1       157751.921875\n",
            "2       182786.390625\n",
            "3       199803.453125\n",
            "4       193856.546875\n",
            "            ...      \n",
            "1454     81599.390625\n",
            "1455     75675.437500\n",
            "1456    179109.875000\n",
            "1457    115265.921875\n",
            "1458    235890.734375\n",
            "Name: SalePrice, Length: 1459, dtype: float32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_b5053406-d04f-4326-9f1f-2616265018de\", \"submission.csv\", 21616)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bArytmJtQtgZ"
      },
      "source": [
        "Lowest score model: 0.1462 is this one!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXNVzQz9Up7P"
      },
      "source": [
        "A file, `submission.csv` will be generated by the code above (CSV is one of the file formats accepted by Kaggle).  Next, we can submit our predictions on Kaggle and compare them to the actual house price (label) on the testing data set, checking for errors. The steps are quite simple:\n",
        "\n",
        "* Log in to the Kaggle website and visit the House Price Prediction Competition page.\n",
        "* Click the “Submit Predictions” or “Late Submission” button on the right.\n",
        "* Click the “Upload Submission File” button in the dashed box at the bottom of the page and select the prediction file you wish to upload.\n",
        "* Click the “Make Submission” button at the bottom of the page to view your results.\n",
        "\n",
        "![](kaggle_submit2.png)\n",
        "\n",
        "## Hints\n",
        "\n",
        "1. Can you improve your model by minimizing the log-price directly? What happens if you try to predict the log price rather than the price?\n",
        "1. Is it always a good idea to replace missing values by their mean? Hint - can you construct a situation where the values are not missing at random?\n",
        "1. Find a better representation to deal with missing values. Hint - What happens if you add an indicator variable?\n",
        "1. Improve the score on Kaggle by tuning the hyperparameters through k-fold crossvalidation.\n",
        "1. Improve the score by improving the model (layers, regularization, dropout).\n",
        "1. What happens if we do not standardize the continuous numerical features like we have done in this section?\n",
        "\n",
        "Note for converting this notebook into PDF. If you use 'File -> Download as -> PDF', you may get the error that svg cannot converted because inkscape is not installed and cannot find PNG images. The easiest way is printing this notebook as a PDF in your browser. Or, you can install inkscape to convert SVG (On macOS, you may `brew cask install xquartz inkscape`, on Ubuntu, you may `sudo apt-get install inkscape`) and change the image URL to local filenames. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJxep9G5wXx-"
      },
      "source": [
        "## All of the model construction and hyper-parameter tuning in this this homework was done by me alone. I discussed my results and approaches with one of my classmates: Nate Fulmer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJKmC3dd83Ce"
      },
      "source": [
        "![doc](https://drive.google.com/uc?id=1DGRgeqNYh6ErWJKB1drAPtkpS5ct0qwT)"
      ]
    }
  ]
}